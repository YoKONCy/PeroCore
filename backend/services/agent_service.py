# Copyright (c) 2026 YoKONCy. All rights reserved.
# This software is licensed under the GNU General Public License v3.0.
# Any unauthorized commercial use or closed-source redistribution is a direct violation of the GPL-3.0 license.
# Original Repository: https://github.com/YoKONCy/PeroCore

import json
import re
import os
import tempfile
import asyncio
import uuid
from datetime import datetime
from typing import List, Dict, AsyncIterable, Any, Optional
from sqlmodel.ext.asyncio.session import AsyncSession
from services.memory_service import MemoryService
from services.llm_service import LLMService
from services.prompt_service import PromptManager
from services.scorer_service import ScorerService
from services.mcp_service import McpClient
from services.mdp.manager import MDPManager
from services.preprocessor.manager import PreprocessorManager
from services.preprocessor.implementations import (
    UserInputPreprocessor,
    HistoryPreprocessor,
    RAGPreprocessor,
    GraphFlashbackPreprocessor,
    ConfigPreprocessor,
    SystemPromptPreprocessor
)
from services.postprocessor.manager import PostprocessorManager
from services.postprocessor.implementations import NITFilterPostprocessor, ThinkingFilterPostprocessor
from core.nit_manager import get_nit_manager
from models import Config, Memory, PetState, ScheduledTask, AIModelConfig, MCPConfig
from sqlmodel import select, desc
from nit_core.tools import TOOLS_MAPPING, TOOLS_DEFINITIONS, plugin_manager
from nit_core.tools.core.ScreenVision.screen_ocr import get_screenshot_base64, save_screenshot
from nit_core.tools.core.SessionOps.session_ops import set_current_session_context
from nit_core.tools.core.WindowsOps.windows_ops import get_active_windows
from nit_core.security import NITSecurityManager

from services.task_manager import task_manager

class AgentService:
    def __init__(self, session: AsyncSession):
        self.session = session
        set_current_session_context(session) # Inject session for tool ops
        self.memory_service = MemoryService()
        self.scorer_service = ScorerService(session)
        self.prompt_manager = PromptManager()
        
        # Initialize Reflection MDP
        reflection_mdp_dir = os.path.join(os.path.dirname(__file__), "mdp", "reflection")
        self.reflection_mdp = MDPManager(reflection_mdp_dir)
        
        # Initialize Helper MDP
        helper_mdp_dir = os.path.join(os.path.dirname(__file__), "mdp", "helper")
        self.helper_mdp = MDPManager(helper_mdp_dir)
        
        # Initialize Preprocessor Pipeline
        self.preprocessor_manager = PreprocessorManager()
        self.preprocessor_manager.register(UserInputPreprocessor())
        self.preprocessor_manager.register(HistoryPreprocessor())
        self.preprocessor_manager.register(RAGPreprocessor())
        self.preprocessor_manager.register(GraphFlashbackPreprocessor())
        self.preprocessor_manager.register(ConfigPreprocessor())
        self.preprocessor_manager.register(SystemPromptPreprocessor())

        # Initialize Postprocessor Pipeline
        self.postprocessor_manager = PostprocessorManager()
        self.postprocessor_manager.register(NITFilterPostprocessor())
        self.postprocessor_manager.register(ThinkingFilterPostprocessor())

    def _log_to_file(self, msg: str):
        try:
            # Use absolute path to ensure log file is created in backend directory
            data_dir = os.environ.get("PERO_DATA_DIR", os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            log_path = os.path.join(data_dir, "debug_vision.log")
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"{datetime.now()} {msg}\n")
        except Exception as e:
            print(f"Failed to write to log file: {e}")

    async def _get_reflection_config(self) -> Dict[str, Any]:
        """è·å–åæ€æ¨¡å‹é…ç½®"""
        configs = {c.key: c.value for c in (await self.session.exec(select(Config))).all()}
        
        if configs.get("reflection_enabled") != "true":
            return None

        reflection_model_id = configs.get("reflection_model_id")
        if not reflection_model_id:
            return None

        try:
            model_config = await self.session.get(AIModelConfig, int(reflection_model_id))
            if not model_config:
                return None
            
            global_api_key = configs.get("global_llm_api_key", "")
            global_api_base = configs.get("global_llm_api_base", "https://api.openai.com")

            final_api_key = model_config.api_key if model_config.provider_type == 'custom' else global_api_key
            final_api_base = model_config.api_base if model_config.provider_type == 'custom' else global_api_base

            return {
                "api_key": final_api_key,
                "api_base": final_api_base,
                "model": model_config.model_id,
                "temperature": 0.1, # åæ€éœ€è¦ç†æ€§ï¼Œä½æ¸©
                "enable_vision": model_config.enable_vision
            }
        except Exception as e:
            print(f"Error getting reflection config: {e}")
            return None

    async def _analyze_file_results_with_aux(self, user_query: str, file_results: List[str]) -> Optional[str]:
        """
        ä½¿ç”¨è¾…åŠ©æ¨¡å‹åˆ†ææ–‡ä»¶æœç´¢ç»“æœ
        """
        try:
            # 1. æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è¾…åŠ©æ¨¡å‹
            aux_model_config = (await self.session.exec(
                select(AIModelConfig).where(AIModelConfig.name == "è¾…åŠ©æ¨¡å‹")
            )).first()
            
            if not aux_model_config:
                print("[Agent] No auxiliary model configured, skipping analysis.")
                return None

            print(f"[Agent] Using auxiliary model ({aux_model_config.model_id}) to analyze search results...")
            
            # 2. å‡†å¤‡ Prompt
            # é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥é¿å… Context Window çˆ†ç‚¸
            preview_files = file_results[:50] 
            files_text = "\n".join(preview_files)
            
            system_prompt = self.helper_mdp.render("file_analysis")
            
            user_prompt = (
                f"ç”¨æˆ·è¯·æ±‚: {user_query}\n\n"
                f"æœç´¢åˆ°çš„æ–‡ä»¶åˆ—è¡¨ (å‰ {len(preview_files)} ä¸ª):\n{files_text}\n\n"
                "è¯·åˆ†æå“ªäº›æ–‡ä»¶æœ€å¯èƒ½æ˜¯ç”¨æˆ·æƒ³è¦çš„ï¼Ÿ"
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # 3. è°ƒç”¨è¾…åŠ©æ¨¡å‹
            # æ„é€ è¾…åŠ© LLMService å®ä¾‹
            # æ³¨æ„ï¼šéœ€è¦ä» Config ä¸­è·å–å…¨å±€ API Key/Base å¦‚æœè¾…åŠ©æ¨¡å‹é…ç½®ä¸ºä½¿ç”¨å…¨å±€
            configs = {c.key: c.value for c in (await self.session.exec(select(Config))).all()}
            global_api_key = configs.get("global_llm_api_key", "")
            global_api_base = configs.get("global_llm_api_base", "https://api.openai.com")

            aux_api_key = aux_model_config.api_key if aux_model_config.provider_type == 'custom' else global_api_key
            aux_api_base = aux_model_config.api_base if aux_model_config.provider_type == 'custom' else global_api_base

            aux_llm = LLMService(
                api_key=aux_api_key,
                api_base=aux_api_base,
                model=aux_model_config.model_id
            )

            # è°ƒç”¨ chat æ–¹æ³•
            response = await aux_llm.chat(messages, temperature=0.3)
            return response["choices"][0]["message"]["content"]
                
        except Exception as e:
            print(f"[Agent] Error in aux analysis: {e}")
            return None
                
            return response_text

        except Exception as e:
            print(f"[Agent] Auxiliary analysis failed: {e}")
            return None

    async def _analyze_screen_with_mcp(self, mcp_client: Optional[McpClient] = None) -> Optional[str]:
        """é€šè¿‡ MCP è°ƒç”¨è§†è§‰æ¨¡å‹åˆ†æå½“å‰å±å¹•"""
        print("\n[Vision] Starting screen analysis...", flush=True)
        
        self._log_to_file("Starting screen analysis")
        
        # å¦‚æœå¤–éƒ¨æ²¡æœ‰ä¼ å…¥ mcp_clientï¼Œåˆ™å°è¯•ä»å·²å¯ç”¨çš„å®¢æˆ·ç«¯ä¸­å¯»æ‰¾å…·å¤‡è§†è§‰èƒ½åŠ›çš„
        if not mcp_client:
            try:
                clients = await self._get_mcp_clients()
                for client in clients:
                    # ç®€å•æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦åŒ…å«è§†è§‰å·¥å…·å…³é”®è¯
                    try:
                        tools = await client.list_tools()
                        vision_keywords = ["vision", "analyze_image", "screen_analysis", "describe_image", "see_screen", "screenshot_analysis", "ocr"]
                        if any(any(k in t["name"].lower() for k in vision_keywords) for t in tools):
                            mcp_client = client
                            print(f"[Vision] Found vision-capable client: {client.name}")
                            break
                    except:
                        continue
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé€€è€Œæ±‚å…¶æ¬¡ç”¨ç¬¬ä¸€ä¸ª
                if not mcp_client and clients:
                    mcp_client = clients[0]
                    print(f"[Vision] No specific vision client found, using first available: {mcp_client.name}")
            except Exception as e:
                msg = f"[Vision] Failed to get MCP clients: {e}"
                print(msg, flush=True)
                self._log_to_file(msg)
                return f"âŒ è§†è§‰åŠŸèƒ½ä¸å¯ç”¨ï¼šè·å– MCP å®¢æˆ·ç«¯å¤±è´¥ ({e})"
            
        if not mcp_client:
            msg = "[Vision] No MCP client configured."
            print(msg, flush=True)
            self._log_to_file(msg)
            # è°ƒè¯•ï¼šæ‰“å°å½“å‰åº“ä¸­æ‰€æœ‰çš„é…ç½®é”®
            try:
                keys = (await self.session.exec(select(Config.key))).all()
                print(f"[Vision] Available config keys: {keys}", flush=True)
            except Exception as e:
                print(f"[Vision] Failed to list config keys: {e}", flush=True)
            return "âŒ è§†è§‰åŠŸèƒ½ä¸å¯ç”¨ï¼šæœªé…ç½® MCP æœåŠ¡å™¨ã€‚è¯·åœ¨è®¾ç½®ä¸­æ·»åŠ æ”¯æŒè§†è§‰èƒ½åŠ›çš„ MCP æœåŠ¡å™¨é…ç½®ï¼ˆå¦‚ GLM-4Vï¼‰ã€‚"

    async def _analyze_screen_with_mcp(self) -> str:
        """
        [å·²å¼ƒç”¨] æ—§ç‰ˆ MCP è§†è§‰åˆ†ææ–¹æ³•
        ç°å·²è¿ç§»è‡³ NIT æ¶æ„ï¼Œæ­¤æ–¹æ³•ä¿ç•™ä»…ä¸ºé˜²æ­¢è¿è¡Œæ—¶ AttributeErrorï¼Œå®é™…ä¸åº”è¢«è°ƒç”¨ã€‚
        """
        return "âš ï¸ æ­¤åŠŸèƒ½å·²è¿ç§»è‡³ NIT æ’ä»¶ç³»ç»Ÿã€‚"

    async def _run_reflection(self, task: str, history: str, screenshot_base64: str = None) -> str:
        """è¿è¡Œåæ€é€»è¾‘"""
        config = await self._get_reflection_config()
        if not config:
            return None
            
        print("[Reflection] Triggering reflection...")
        
        # è§†è§‰åˆ†æå·²è¿ç§»è‡³ NITï¼Œåæ€é€»è¾‘æš‚ä¸å¼ºä¾èµ–è§†è§‰é¢„åˆ†æ
        vision_analysis = None
        is_blind = not config.get("enable_vision")

        llm = LLMService(
            api_key=config.get("api_key"),
            api_base=config.get("api_base"),
            model=config.get("model")
        )
        
        # æ ¹æ®è§†è§‰èƒ½åŠ›çŠ¶æ€åŠ¨æ€è°ƒæ•´ Prompt
        vision_prompt_name = "vision_enabled" if not is_blind else "vision_disabled"
        vision_instruction_block = "{{" + vision_prompt_name + "}}"

        # ç”Ÿæˆå·¥å…·åˆ—è¡¨å­—ç¬¦ä¸²
        tools_list_str = ""
        # åŠ¨æ€è·å–æœ€æ–°å·¥å…·å®šä¹‰
        # current_tools_defs = plugin_manager.get_all_definitions() 
        
        # æ”¹ç”¨ç¬¦åˆ NIT åè®®çš„ç­›é€‰é€»è¾‘ï¼šå¿…é¡»ç»è¿‡ NIT Manager çš„å¯ç”¨æ£€æŸ¥
        nit_manager = get_nit_manager()
        current_tools_defs = []
        
        for plugin_name, manifest in plugin_manager.plugins.items():
            # 1. æ£€æŸ¥åˆ†ç±»å¼€å…³ (Level 1)
            category = manifest.get("_category", "plugins") # é»˜è®¤ä¸º plugins
            if not nit_manager.is_category_enabled(category):
                continue
                
            # 2. æ£€æŸ¥æ’ä»¶å¼€å…³ (Level 2)
            if not nit_manager.is_plugin_enabled(plugin_name):
                continue
            
            # 3. æå–å·¥å…·å®šä¹‰
            if "capabilities" in manifest and "invocationCommands" in manifest["capabilities"]:
                current_tools_defs.extend(manifest["capabilities"]["invocationCommands"])
            elif "capabilities" in manifest and "toolDefinitions" in manifest["capabilities"]:
                current_tools_defs.extend(manifest["capabilities"]["toolDefinitions"])
        
        for tool_def in current_tools_defs:
            if "function" in tool_def:
                func = tool_def["function"]
                name = func.get("name", "unknown")
                desc = func.get("description", "No description")
                tools_list_str += f"- {name}: {desc}\n"
            elif "name" in tool_def: # æ”¯æŒ NIT é£æ ¼çš„å®šä¹‰
                name = tool_def.get("name", "unknown")
                desc = tool_def.get("description", "No description")
                tools_list_str += f"- {name}: {desc}\n"
            elif "commandIdentifier" in tool_def: # æ”¯æŒ NIT 2.0 é£æ ¼å®šä¹‰
                name = tool_def.get("commandIdentifier", "unknown")
                desc = tool_def.get("description", "No description")
                tools_list_str += f"- {name}: {desc}\n"

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ UI è‡ªåŠ¨åŒ–æ“ä½œåæ€åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå½“å‰çš„æ“ä½œç»“æœæ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œä»¥åŠæ˜¯å¦é™·å…¥äº†æ­»å¾ªç¯ã€‚

**å¯ç”¨å·¥å…·åˆ—è¡¨**:
{tools_list_str}

**Prompt Debug Info**:
Tool List Length: {len(tools_list_str)}

è¯·ä»”ç»†å¯¹æ¯”ç”¨æˆ·çš„ä»»åŠ¡ç›®æ ‡å’Œå½“å‰çš„è¿‘æœŸæ“ä½œå†å²ã€‚
1. **å·¥å…·å‚æ•°æ ¡éªŒ**: æ£€æŸ¥ä¸Šä¸€æ­¥å·¥å…·è°ƒç”¨æ˜¯å¦å› ä¸ºå‚æ•°é”™è¯¯ï¼ˆå¦‚ browser_click è¯¯ä¼ äº† url å‚æ•°ï¼‰è€Œå¤±è´¥ã€‚è¯·åŠ¡å¿…æ ¹æ®ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦å­˜åœ¨ã€‚
{vision_instruction}
3. **å†³ç­–å»ºè®®**: å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºåŸå› å¹¶ç»™å‡ºä¿®æ­£å»ºè®®ã€‚
   - **ä¸¥ç¦è‡†é€ ä¸å­˜åœ¨çš„å·¥å…·**ï¼ˆå¦‚ `browser_search`ï¼‰ï¼Œåªèƒ½ä»ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘ä¸­é€‰æ‹©ã€‚
   - å¦‚æœç”¨æˆ·æƒ³æœç´¢ï¼Œåº”å»ºè®®ä½¿ç”¨ `browser_open_url` æ‰“å¼€æœç´¢å¼•æ“ï¼Œç„¶åä½¿ç”¨ `browser_input` å’Œ `browser_click`ã€‚
   - å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œåªéœ€å›ç­”"NORMAL"ã€‚
"""
        user_content = f"ä»»åŠ¡ç›®æ ‡: {task}\n\nè¿‘æœŸæ“ä½œå†å²:\n{history}"
        
        if vision_analysis:
            user_content += f"\n\n[å½“å‰å±å¹•è§†è§‰åˆ†æ]:\n{vision_analysis}"
            print(f"[Reflection] Added vision analysis to context.")
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # ç»„è£…ç”¨æˆ·æ¶ˆæ¯å†…å®¹
        content = [{"type": "text", "text": user_content}]
        
        # å¦‚æœæä¾›äº†æˆªå›¾ä¸”æ¨¡å‹æ”¯æŒå¤šæ¨¡æ€ï¼Œåˆ™æ³¨å…¥å›¾ç‰‡
        if screenshot_base64 and config.get("enable_vision"):
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{screenshot_base64}"}
            })
            print(f"[Reflection] Injected screenshot into reflection context.")
        
        messages.append({"role": "user", "content": content})

        try:
            response = await llm.chat(messages, temperature=config.get("temperature", 0.7))
            content = response["choices"][0]["message"]["content"]
            print(f"[Reflection] Result: {content}")
            return content
        except Exception as e:
            print(f"[Reflection] Error: {e}")
            return None

    async def _get_llm_config(self) -> Dict[str, Any]:
        # 1. è·å–å…¨å±€é…ç½®
        configs = {c.key: c.value for c in (await self.session.exec(select(Config))).all()}
        
        global_api_key = configs.get("global_llm_api_key", "")
        global_api_base = configs.get("global_llm_api_base", "https://api.openai.com")
        current_model_id = configs.get("current_model_id")

        # é»˜è®¤å›é€€é…ç½®
        fallback_config = {
            "api_key": global_api_key or configs.get("ppc.apiKey", ""), # å…¼å®¹æ—§key
            "api_base": global_api_base or configs.get("ppc.apiBase", "https://api.openai.com"),
            "model": configs.get("ppc.modelName", "gpt-3.5-turbo"),
            "temperature": 0.7,
            "enable_vision": False
        }

        # 2. å¦‚æœæ²¡æœ‰é€‰ä¸­æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤/å›é€€é…ç½®
        if not current_model_id:
            return fallback_config

        # 3. è·å–é€‰ä¸­æ¨¡å‹å¡ç‰‡
        try:
            model_config = await self.session.get(AIModelConfig, int(current_model_id))
            if not model_config:
                return fallback_config
        except ValueError:
            return fallback_config

        # 4. ç»„è£…é…ç½®
        final_api_key = model_config.api_key if model_config.provider_type == 'custom' else global_api_key
        final_api_base = model_config.api_base if model_config.provider_type == 'custom' else global_api_base
        
        return {
            "api_key": final_api_key,
            "api_base": final_api_base,
            "model": model_config.model_id,
            "temperature": model_config.temperature,
            "enable_vision": model_config.enable_vision
        }

    async def _get_pet_state(self) -> PetState:
        state = (await self.session.exec(select(PetState).order_by(desc(PetState.updated_at)).limit(1))).first()
        if not state:
            state = PetState()
            self.session.add(state)
            await self.session.commit()
            await self.session.refresh(state)
        return state

    async def handle_proactive_observation(self, intent_description: str, score: float):
        """
        Handle a proactive visual observation from AuraVision.
        """
        # 1. Check if we should talk now
        # TODO: Implement more complex gating (e.g. check last talk time)
        print(f"[Agent] Proactive observation received: {intent_description} (Score: {score:.4f})")
        
        # 2. Construct an internal sensing prompt
        internal_prompt = f"""
[PERO_INTERNAL_SENSE]
è§†è§‰æ„å›¾: "{intent_description}"
ç½®ä¿¡åº¦: {score:.4f}

è¯·è§‚å¯Ÿå½“å‰ç¯å¢ƒå’Œä½ çš„è®°å¿†ã€‚å¦‚æœä½ è§‰å¾—ç°åœ¨æ˜¯ä¸ä¸»äººè¯´è¯çš„å¥½æ—¶æœºï¼Œè¯·ç«‹å³è¡ŒåŠ¨ã€‚
å¦‚æœä¸»äººæ­£å¿™æˆ–ä½ æ²¡æœ‰ä»€ä¹ˆæœ‰æ„ä¹‰çš„è¯è¦è¯´ï¼Œè¯·ä¿æŒå®‰é™ï¼ˆè¾“å‡º <NOTHING>ï¼‰ã€‚
"""
        # 3. Trigger a special session
        # This would involve calling self.process_request with a pseudo-user message
        # but marked as an internal trigger.
        pass

    async def _get_mcp_clients(self) -> List[McpClient]:
        """è·å–æ‰€æœ‰å·²å¯ç”¨çš„ MCP å®¢æˆ·ç«¯"""
        clients = []
        # 1. å°è¯•ä»æ–°ç‰ˆé€šç”¨ MCP é…ç½®è¡¨ä¸­è·å–
        try:
            # è·å–æ‰€æœ‰é…ç½®ï¼Œæ— è®ºæ˜¯å¦å¯ç”¨ï¼Œä»¥æ­¤åˆ¤æ–­æ–°è¡¨æ˜¯å¦æœ‰æ•°æ®
            all_mcp_configs = (await self.session.exec(select(MCPConfig))).all()
            
            if all_mcp_configs:
                print(f"[AgentService] Found {len(all_mcp_configs)} configs in MCPConfig table. Using as source of truth.")
                for mcp_config_obj in all_mcp_configs:
                    if not mcp_config_obj.enabled:
                        continue
                        
                    print(f"[AgentService] Loading enabled MCP config: {mcp_config_obj.name}")
                    client_config = {
                        "type": mcp_config_obj.type,
                        "name": mcp_config_obj.name
                    }
                    
                    if mcp_config_obj.type == "stdio":
                        client_config.update({
                            "command": mcp_config_obj.command,
                            "args": json.loads(mcp_config_obj.args or "[]"),
                            "env": json.loads(mcp_config_obj.env or "{}")
                        })
                    elif mcp_config_obj.type == "sse":
                        client_config.update({
                            "url": mcp_config_obj.url
                        })
                    
                    clients.append(McpClient(config=client_config))
                # åªè¦æ–°è¡¨æœ‰æ•°æ®ï¼ˆå³ä½¿å…¨éƒ¨è¢«ç¦ç”¨ï¼‰ï¼Œå°±ä»¥æ­¤ä¸ºå‡†ï¼Œä¸å†å‘ä¸‹å›é€€
                return clients
        except Exception as e:
            print(f"[AgentService] Error querying MCPConfig table: {e}")

        # åªæœ‰å½“æ–°è¡¨å®Œå…¨æ²¡æ•°æ®æ—¶ï¼Œæ‰å°è¯•è·å–æ—§ç‰ˆé…ç½®ä½œä¸ºå›é€€
        # 2. å°è¯•è·å–å®Œæ•´ JSON é…ç½®
            try:
                json_config = (await self.session.exec(select(Config).where(Config.key == "mcp_config_json"))).first()
                
                if json_config and json_config.value:
                    try:
                        config_data = json.loads(json_config.value)
                        if "mcpServers" in config_data:
                            for name, server_config in config_data["mcpServers"].items():
                                # æ£€æŸ¥æ˜¯å¦å¯ç”¨ (é»˜è®¤ä¸º True)
                                if not server_config.get("enabled", True):
                                    print(f"[AgentService] Skipping disabled MCP JSON config for server: {name}")
                                    continue
                                    
                                print(f"[AgentService] Found MCP JSON config for server: {name}")
                                # ç¡®ä¿é…ç½®ä¸­æœ‰åå­—
                                if "name" not in server_config:
                                    server_config["name"] = name
                                clients.append(McpClient(config=server_config))
                        else:
                            print(f"[AgentService] Found direct MCP JSON config")
                            clients.append(McpClient(config=config_data))
                    except Exception as e:
                        print(f"[AgentService] Failed to load MCP JSON config: {e}")
            except Exception as e:
                print(f"[AgentService] Error querying mcp_config_json: {e}")

            # 3. å›é€€åˆ°æ—§çš„ URL/Key é…ç½® (ä»…å½“ä»æ²¡æœ‰å®¢æˆ·ç«¯æ—¶)
            if not clients:
                try:
                    url_config = (await self.session.exec(select(Config).where(Config.key == "mcp_server_url"))).first()
                    
                    if url_config and url_config.value:
                        key_config = (await self.session.exec(select(Config).where(Config.key == "mcp_api_key"))).first()
                        api_key = key_config.value if key_config else None
                        
                        print(f"[AgentService] Falling back to old MCP URL config: {url_config.value}")
                        clients.append(McpClient(config={
                            "type": "sse",
                            "url": url_config.value,
                            "api_key": api_key,
                            "name": "Legacy-MCP"
                        }))
                except Exception as e:
                    print(f"[AgentService] Error querying mcp_server_url: {e}")

        return clients

    async def _save_parsed_metadata(self, text: str, source: str = "desktop", mcp_clients: List[McpClient] = None, execute_nit: bool = True, expected_nit_id: str = None) -> List[Dict[str, Any]]:
        """è§£æå¹¶ä¿å­˜ LLM è¿”å›çš„å…ƒæ•°æ®ã€‚ç°åœ¨ä¸»è¦è´Ÿè´£ NIT å·¥å…·è°ƒç”¨ã€‚"""
        try:
            # 1. å¤„ç† NIT å·¥å…·è°ƒç”¨ (æ ¸å¿ƒé€»è¾‘)
            nit_results = []
            if execute_nit:
                # --- [Security Gate] é’ˆå¯¹æ‰‹æœºç«¯çš„ NIT è„šæœ¬ç¡¬éš”ç¦» ---
                if source == "mobile":
                    sensitive_tool_keywords = ["screenshot", "screen", "windows", "shell", "cmd", "file", "app", "browser", "exec", "write"]
                    # æ£€æŸ¥ text ä¸­æ˜¯å¦åŒ…å« <nit> ä¸”å†…å®¹æ¶‰åŠæ•æ„Ÿè¯
                    if "<nit" in text and any(kw in text.lower() for kw in sensitive_tool_keywords):
                        print(f"[ğŸ›¡ï¸ Hard Security] Blocked NIT script execution from mobile: {text[:50]}...")
                        return [{"status": "error", "message": "Permission Denied: NIT script contains restricted tools for mobile source."}]

                from nit_core.dispatcher import get_dispatcher
                nit_dispatcher = get_dispatcher()
                
                # å‡†å¤‡ MCP æ’ä»¶ (å¦‚æœå­˜åœ¨)
                extra_plugins = None
                if mcp_clients:
                    try:
                        from nit_core.bridge import NITBridge
                        bridge = NITBridge(nit_dispatcher)
                        extra_plugins = await bridge.get_mcp_plugins(mcp_clients)
                    except Exception as e:
                        print(f"[Agent] Failed to bridge MCP tools to NIT: {e}")

                nit_results = await nit_dispatcher.dispatch(text, extra_plugins=extra_plugins, expected_nit_id=expected_nit_id)
                
                if nit_results:
                    print(f"[Agent] Executed {len(nit_results)} NIT tool calls")

            # 2. ä¼ ç»Ÿ XML æ ‡ç­¾è§£æ (å·²å¼ƒç”¨ï¼Œä»…ä¿ç•™æ¡†æ¶ä»¥é˜²æœªæ¥æ‰©å±•)
            # æ³¨æ„ï¼šçŠ¶æ€æ›´æ–° (PEROCUE, CLICK_MESSAGES ç­‰) å·²è¿ç§»è‡³ UpdateStatusPlugin (NIT)
            # é•¿è®°å¿† (MEMORY) å·²ç”± ScorerService ç‹¬ç«‹å¤„ç†
            
            await self.session.commit()
            return nit_results
        except Exception as e:
            await self.session.rollback()
            print(f"Error in _save_parsed_metadata: {e}")
            return []

    async def social_chat(self, messages: List[Dict[str, Any]], session_id: str) -> str:
        """
        Specialized chat mode for Social Interactions (QQ/OneBot).
        - Uses isolated session history.
        - Restricted toolset (Safe tools only).
        - Returns the final response text directly.
        """
        print(f"[SocialAgent] Processing social chat for session: {session_id}")
        
        # 1. Get Config (Use global config or specific social model)
        config = await self._get_llm_config()
        
        # 2. Prepare Tools (Whitelist Strategy)
        social_tools = []
        try:
            # We want to filter tools that are specifically for social mode.
            # Assuming PluginManager has loaded them and they are available in plugin_manager.
            all_tools = plugin_manager.get_all_definitions()
            
            # Whitelist of safe prefixes/names
            safe_prefixes = ["qq_"]
            safe_names = ["read_social_memory", "read_agent_memory", "notify_master"]
            
            for tool_def in all_tools:
                t_name = ""
                if "function" in tool_def:
                    t_name = tool_def["function"].get("name", "")
                elif "name" in tool_def:
                    t_name = tool_def.get("name", "")
                
                if any(t_name.startswith(p) for p in safe_prefixes) or t_name in safe_names:
                    social_tools.append(tool_def)
                    
            print(f"[SocialAgent] Loaded {len(social_tools)} social tools.")
        except Exception as e:
            print(f"[SocialAgent] Error loading tools: {e}")
            social_tools = []

        # 3. Call LLM
        llm = LLMService(
            api_key=config.get("api_key"),
            api_base=config.get("api_base"),
            model=config.get("model")
        )
        
        # We use a simplified loop for social mode (no complex reflection/vision for now)
        try:
            # Non-streaming call for simplicity in Phase 2 MVP
            response = await llm.chat(messages, temperature=0.7, tools=social_tools if social_tools else None)
            
            response_msg = response["choices"][0]["message"]
            content = response_msg.get("content", "")
            tool_calls = response_msg.get("tool_calls", [])
            
            # 4. Handle Tool Calls (Simple Loop)
            # If tool calls exist, execute them and recurse (limit 3 turns)
            # For MVP Phase 2, let's just execute and return the result or confirmation.
            
            if tool_calls:
                print(f"[SocialAgent] LLM requested {len(tool_calls)} tool calls.")
                # Append assistant message with tool calls
                messages.append(response_msg)
                
                for tc in tool_calls:
                    func_name = tc["function"]["name"]
                    args_str = tc["function"]["arguments"]
                    call_id = tc["id"]
                    
                    print(f"[SocialAgent] Executing tool: {func_name}")
                    
                    # Execute via NIT Dispatcher
                    from nit_core.dispatcher import get_dispatcher
                    dispatcher = get_dispatcher()
                    
                    # We need to construct the command string for dispatcher or call function directly
                    # Since dispatcher parses text, we might need to find the function in plugin_manager map
                    # Let's try to use plugin_manager directly or execute the python function if we can find it.
                    # Actually, AgentService._save_parsed_metadata uses dispatcher.dispatch(text).
                    # But here we have structured tool calls.
                    
                    # Quick fix: Use the tools_map in plugin_manager if available, or just use dispatcher's execute_tool if exposed.
                    # Looking at AgentService, it seems standard tool execution is manual in the loop.
                    
                    # Let's verify if we can just invoke the function from the tools definitions?
                    # No, definitions are JSON.
                    
                    # Let's use the PluginManager's tools_map which maps name -> callable
                    func = plugin_manager.tools_map.get(func_name)
                    tool_result = ""
                    
                    if func:
                        try:
                            import inspect
                            args = json.loads(args_str)
                            if inspect.iscoroutinefunction(func):
                                tool_result = await func(**args)
                            else:
                                tool_result = func(**args)
                        except Exception as e:
                            tool_result = f"Error executing {func_name}: {e}"
                    else:
                        tool_result = f"Tool {func_name} not found."
                        
                    # Append Tool Result
                    messages.append({
                        "tool_call_id": call_id,
                        "role": "tool",
                        "name": func_name,
                        "content": str(tool_result)
                    })
                    
                # Recursive call (Second turn)
                # For safety, just one recursion depth for now
                print("[SocialAgent] Sending tool results back to LLM...")
                response_2 = await llm.chat(messages, temperature=0.7, tools=social_tools)
                content = response_2["choices"][0]["message"].get("content", "")
                
            return content

        except Exception as e:
            print(f"[SocialAgent] Error: {e}")
            return f"[System Error] {str(e)}"

    async def _run_scorer_background(self, user_msg: str, assistant_msg: str, source: str, pair_id: str = None):
        """åå°è¿è¡Œ Scorer æœåŠ¡ï¼Œä½¿ç”¨ç‹¬ç«‹ Session"""
        from database import engine
        from sqlmodel.ext.asyncio.session import AsyncSession
        from sqlalchemy.orm import sessionmaker
        
        try:
            async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
            async with async_session() as session:
                scorer = ScorerService(session)
                await scorer.process_interaction(user_msg, assistant_msg, source, pair_id=pair_id)
        except Exception as e:
            print(f"[Agent] åå°ç§˜ä¹¦æœåŠ¡å¤±è´¥: {e}")

    async def _trigger_dream(self):
        """åå°è§¦å‘æ¢¦å¢ƒæœºåˆ¶"""
        try:
            from services.reflection_service import ReflectionService
            from database import engine
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.orm import sessionmaker
            import random
            
            print("[Agent] Spawning background dream task...", flush=True)
            async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
            async with async_session() as session:
                # Update last trigger time in Config
                now_str = datetime.now().isoformat()
                config_last_dream = await session.get(Config, "last_dream_trigger_time")
                if not config_last_dream:
                    config_last_dream = Config(key="last_dream_trigger_time", value=now_str)
                    session.add(config_last_dream)
                else:
                    config_last_dream.value = now_str
                    config_last_dream.updated_at = datetime.now()
                await session.commit()

                service = ReflectionService(session)

                # 1. è¡¥å½•è®°å¿† (Priority: Fix failures first)
                await service.backfill_failed_scorer_tasks()
        
                # 2. å­¤ç‹¬è®°å¿†æ‰«æ (New Feature: Fix isolated memories)
                # æ¯æ¬¡æ¢¦å¢ƒå‘¨æœŸæ‰«æ 3 ä¸ªå­¤ç‹¬è®°å¿†
                await service.scan_lonely_memories(limit=3)

                # 3. å…³è”æŒ–æ˜ (High Priority)
                await service.dream_and_associate(limit=10)
                
                # 3. è®°å¿†å‹ç¼© (Low Priority, 10% chance)
                if random.random() < 0.1:
                    # é»˜è®¤é…ç½®: å‹ç¼©3å¤©å‰çš„ä½ä»·å€¼è®°å¿†
                    await service.consolidate_memories(lookback_days=3, importance_threshold=4)

        except Exception as e:
            print(f"[Agent] Background dream failed: {e}")

    async def chat(self, messages: List[Dict[str, Any]], source: str = "desktop", session_id: str = "default", on_status: Optional[Any] = None, is_voice_mode: bool = False, user_text_override: str = None, skip_save: bool = False, system_trigger_instruction: str = None) -> AsyncIterable[str]:
        # [NIT Security] Generate ID for this request context
        current_nit_id = NITSecurityManager.generate_random_id()
        
        # Notify CompanionService of user activity to prevent interruption
        try:
            from services.companion_service import companion_service
            companion_service.update_activity()
        except ImportError:
            pass
        except Exception as e:
            print(f"[Agent] Failed to update companion activity: {e}")
            
        # Cancel any pending 'reaction' tasks because user is interacting
        if not system_trigger_instruction:
            try:
                # Assuming 'reaction' type tasks are those that should be cancelled on interaction
                statement = select(ScheduledTask).where(ScheduledTask.type == "reaction").where(ScheduledTask.is_triggered == False)
                tasks_to_cancel = (await self.session.exec(statement)).all()
                if tasks_to_cancel:
                    print(f"[Agent] User interaction detected. Cancelling {len(tasks_to_cancel)} pending reaction tasks.")
                    for t in tasks_to_cancel:
                        await self.session.delete(t)
                    await self.session.commit()
            except Exception as e:
                print(f"[Agent] Failed to cancel reaction tasks: {e}")

        # [Work Mode] Session Override
        # Check if we are in an active work session. If so, override the session_id to isolate history.
        # [Fix] Disabled global override to prevent hijacking of 'default' (Pet) session. 
        # IdeChat handles session resolution via ide_router.
        # try:
        #     config_session = (await self.session.exec(select(Config).where(Config.key == "current_session_id"))).first()
        #     if config_session and config_session.value and config_session.value != "default":
        #         original_session_id = session_id
        #         session_id = config_session.value
        #         print(f"[Agent] Work Mode Active: Overriding session_id '{original_session_id}' -> '{session_id}'")
        # except Exception as e:
        #     print(f"[Agent] Failed to check session override: {e}")

        print(f"[Agent] Chat request received. Source: {source}, Msg count: {len(messages)}, Voice: {is_voice_mode}")
        
        # 1. Initialize Context
        context = {
            "messages": messages,
            "source": source,
            "session_id": session_id,
            "session": self.session,
            "memory_service": self.memory_service,
            "prompt_manager": self.prompt_manager,
            "user_text_override": user_text_override,
            "is_voice_mode": is_voice_mode,
            "agent_service": self,
            "variables": {},
            "nit_id": current_nit_id,
        }
        
        # 2. Run Preprocessor Pipeline
        if on_status: await on_status("thinking", "æ­£åœ¨æ•´ç†è®°å¿†å’Œæ€ç»ª...")
        context = await self.preprocessor_manager.process(context)
        
        # 3. Extract Results
        user_message = context.get("user_message", "")
        final_messages = context.get("final_messages", [])
        config = context.get("llm_config", {})
        
        # [Feature] System Trigger Instruction
        if system_trigger_instruction:
            print(f"[Agent] Appending System Trigger Instruction: {system_trigger_instruction}")
            final_messages.append({
                "role": "system",
                "content": system_trigger_instruction
            })
            if not user_message:
                user_message = f"ã€ç³»ç»Ÿè§¦å‘ã€‘{system_trigger_instruction}"

        # [Feature] Mobile Source Awareness
        if source == "mobile":
            mobile_instruction = """
[SYSTEM_NOTE: MOBILE_MODE]
The user is currently communicating with you via a MOBILE device (Peroperochat).
- Desktop-only tools (e.g., open_app, windows_operation, screenshot) will execute on the OWNER'S PC, NOT the mobile device.
- Please AVOID using these tools unless explicitly asked to do something on the PC.
- Focus on natural conversation and emotional interaction.
"""
            final_messages.append({
                "role": "system",
                "content": mobile_instruction
            })
            print("[Agent] Mobile Source Awareness injected.")

        # [Feature] Active Window Injection
        # æ³¨å…¥å½“å‰æ´»è·ƒçª—å£åˆ—è¡¨ï¼Œé˜²æ­¢ AI å¹»è§‰ï¼ˆä»¥ä¸ºåº”ç”¨å·²æ‰“å¼€ï¼‰
        try:
            active_windows = get_active_windows()
            if isinstance(active_windows, list) and active_windows:
                # Limit to avoid token explosion
                window_list_str = "\n".join(active_windows[:20]) 
                if len(active_windows) > 20:
                    window_list_str += f"\n... ({len(active_windows) - 20} more)"
                
                state_msg = f"""<system_state>
Current Active Windows (Taskbar):
{window_list_str}
</system_state>
Instruction: When opening an app, check this list first. If it's already running, use `windows_operation(action="activate", target="Name")` or just interact with it directly."""
                
                # Append to messages (System role)
                final_messages.append({
                    "role": "system",
                    "content": state_msg
                })
        except Exception as e:
            print(f"[Agent] Failed to inject active windows: {e}")
        
        # Fallback if config is missing (should not happen if ConfigPreprocessor runs)
        if not config:
            config = await self._get_llm_config()

        print(f"[Agent] Prompt composed via Preprocessors. Messages count: {len(final_messages)}")

        llm = LLMService(
            api_key=config.get("api_key"),
            api_base=config.get("api_base"),
            model=config.get("model")
        )
        temperature = config.get("temperature", 0.7)
        
        # 5. åˆå¹¶åŠ¨æ€ MCP å·¥å…·
        if on_status: await on_status("thinking", "æ­£åœ¨åŠ è½½å·¥å…·...")
        print("[Agent] Loading MCP tools...")
        
        # --- å·¥å…·åˆ—è¡¨ä¼˜åŒ– ---
        # æ ¹æ®ä¸»æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ï¼ŒåŠ¨æ€è°ƒæ•´å·¥å…·å®šä¹‰
        enable_vision = config.get("enable_vision", False)
        dynamic_tools = []
        for tool_def in TOOLS_DEFINITIONS:
            # æ£€æŸ¥å·¥å…·å®šä¹‰æ˜¯å¦ç¬¦åˆ OpenAI Function Calling æ ¼å¼
            if "function" not in tool_def or "name" not in tool_def.get("function", {}):
                # å¦‚æœä¸æ˜¯æ ‡å‡† Function æ ¼å¼ï¼ˆä¾‹å¦‚çº¯ NIT æŒ‡ä»¤ï¼‰ï¼Œåˆ™è·³è¿‡ Native Tool æ³¨å†Œ
                # ä½†å®ƒä»¬ä¾ç„¶ä¼šåœ¨ System Prompt ä¸­å¯è§ï¼ˆå‰é¢å·²å¤„ç†ï¼‰
                continue

            # å¤åˆ¶ä¸€ä»½ä»¥é˜²ä¿®æ”¹å…¨å±€å˜é‡
            new_tool_def = json.loads(json.dumps(tool_def))
            tool_name = new_tool_def["function"]["name"]
            
            # å®‰å…¨æ ¡éªŒï¼šå¦‚æœæ˜¯æ‰‹æœºç«¯ä¸”åŒ…å«æ•æ„Ÿè¯ï¼Œåˆ™ç›´æ¥å‰”é™¤
            sensitive_tool_keywords = ["screenshot", "screen", "windows", "shell", "cmd", "file", "app", "browser", "exec", "write"]
            if source == "mobile" and any(kw in tool_name.lower() for kw in sensitive_tool_keywords):
                print(f"[Security] Filtering sensitive tool for mobile: {tool_name}")
                continue
            
            # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸”å·¥å…·æ˜¯ screen_ocrï¼Œåˆ™è·³è¿‡ä¸æ³¨å…¥
            if enable_vision and tool_name == "screen_ocr":
                continue
            
            if tool_name == "take_screenshot" or tool_name == "see_screen":
                if not enable_vision:
                    new_tool_def["function"]["description"] = "è·å–å½“å‰å±å¹•çš„è§†è§‰åˆ†ææŠ¥å‘Šã€‚ç³»ç»Ÿå°†è°ƒç”¨è§†è§‰ MCP æœåŠ¡å™¨åˆ†æå±å¹•å†…å®¹å¹¶è¿”å›è¯¦ç»†çš„æ–‡å­—æè¿°ã€‚å½“ä½ éœ€è¦äº†è§£å±å¹•ä¸Šçš„è§†è§‰ä¿¡æ¯ã€æˆ–å‡ºäºå¥½å¥‡æƒ³çœ‹çœ‹ä¸»äººåœ¨åšä»€ä¹ˆä½†æ— æ³•ç›´æ¥çœ‹åˆ°å›¾ç‰‡æ—¶ï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·ã€‚"
                    # éå¤šæ¨¡æ€æ¨¡å¼ä¸‹ï¼Œcount å‚æ•°å¯èƒ½æ²¡æ„ä¹‰ï¼Œæˆ–è€…æˆ‘ä»¬åªæ”¯æŒ 1
                    new_tool_def["function"]["parameters"]["properties"]["count"]["description"] = "è·å–æˆªå›¾å¹¶åˆ†æçš„æ•°é‡ã€‚åœ¨éå¤šæ¨¡æ€æ¨¡å¼ä¸‹ï¼Œå»ºè®®è®¾ä¸º 1ã€‚"
            dynamic_tools.append(new_tool_def)
        
        # Log prepared tools for debugging
        print(f"[AgentService] Prepared {len(dynamic_tools)} tools: {[t['function']['name'] for t in dynamic_tools]}")
        # --- End å·¥å…·åˆ—è¡¨ä¼˜åŒ– ---
        
        mcp_clients = []
        try:
            mcp_clients = await self._get_mcp_clients()
            print(f"[Agent] Loaded {len(mcp_clients)} MCP Clients")
        except Exception as e:
            print(f"[Agent] Failed to get MCP clients: {e}")

        mcp_tool_map = {} # tool_name -> client
        for client in mcp_clients:
            try:
                mcp_tools = await client.list_tools()
                for tool in mcp_tools:
                    tool_name = f"mcp_{tool['name']}"
                    
                    # åŒæ ·å¯¹ MCP å·¥å…·å®æ–½å®‰å…¨æ ¡éªŒ
                    sensitive_tool_keywords = ["screenshot", "screen", "windows", "shell", "cmd", "file", "app", "browser", "exec", "write"]
                    if source == "mobile" and any(kw in tool_name.lower() for kw in sensitive_tool_keywords):
                        print(f"[Security] Filtering sensitive MCP tool for mobile: {tool_name}")
                        continue

                    # å¦‚æœæœ‰é‡åï¼Œåé¢çš„ä¼šè¦†ç›–å‰é¢çš„ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åŠ åç¼€
                    dynamic_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool_name,
                            "description": tool.get("description", ""),
                            "parameters": tool.get("inputSchema", {})
                        }
                    })
                    mcp_tool_map[tool_name] = client
                    print(f"[Agent] Registered MCP tool: {tool_name} from {client.name}")
            except Exception as e:
                print(f"[AgentService] Warning: Failed to list tools for client {client.name}: {e}")

        # --- Native Tools Config ---
        disable_native_tools_config = (await self.session.exec(select(Config).where(Config.key == "disable_native_tools"))).first()
        disable_native_tools = disable_native_tools_config.value.lower() == "true" if disable_native_tools_config else False
        tools_to_pass = None if disable_native_tools else dynamic_tools
        if disable_native_tools:
            print("[Agent] Native Tools (Function Calling) are DISABLED via config.")

        full_response_text = ""
        accumulated_full_response = "" # ç”¨äºä¿å­˜å®Œæ•´çš„å¯¹è¯è®°å½•ï¼ˆåŒ…å«æ‰€æœ‰ ReAct è½®æ¬¡çš„æ€è€ƒè¿‡ç¨‹ï¼‰
        pair_id = str(uuid.uuid4()) # ç”ŸæˆåŸå­æ€§ç»‘å®šID
        
        # --- ReAct Loop Configuration ---
        turn_count = 0
        MAX_TURNS = 30 # [Safety] Limit max turns to prevent infinite loops (was 9999)
        consecutive_error_count = 0 # è¿ç»­é”™è¯¯è®¡æ•°å™¨

        # æ³¨å†Œä»»åŠ¡ç®¡ç†å™¨ (å¦‚æœ session_id å­˜åœ¨)
        if session_id:
            task_manager.register(session_id)
        
        try:
            while turn_count < MAX_TURNS:
                # 1. æ£€æŸ¥æš‚åœçŠ¶æ€
                if session_id:
                    # å¦‚æœæš‚åœï¼Œè¿™é‡Œä¼šé˜»å¡ç­‰å¾…
                    await task_manager.check_pause(session_id)
                    
                    # 2. æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æ³¨å…¥çš„æŒ‡ä»¤
                    injected = task_manager.get_injected_instruction(session_id)
                    if injected:
                        print(f"[Agent] Detected injected instruction: {injected}")
                        final_messages.append({
                            "role": "user", 
                            "content": f"ã€ä¸»äººå³æ—¶æŒ‡ä»¤ã€‘: {injected}"
                        })
                        # æ³¨å…¥æŒ‡ä»¤åï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®æ€è€ƒï¼Œå¸¦ä¸Šæ–°çš„ä¸Šä¸‹æ–‡

                turn_count += 1
                current_turn_text = ""
                has_tool_calls_in_this_turn = False
                collected_tool_calls = [] # ç”¨äºæ”¶é›†æœ¬è½®æµå¼è¿”å›çš„å·¥å…·è°ƒç”¨ç‰‡æ®µ
                
                if on_status: await on_status("thinking", f"æ­£åœ¨æ€è€ƒ (ç¬¬ {turn_count} è½®)...")
                print(f"[Agent] Starting LLM stream (Turn {turn_count})...")
                
                # Define Raw Stream Generator
                async def raw_stream_source():
                    nonlocal current_turn_text, full_response_text, has_tool_calls_in_this_turn, collected_tool_calls
                    
                    async for delta in llm.chat_stream_deltas(final_messages, temperature=temperature, tools=tools_to_pass):
                        # Debug Log: Print delta content
                        # print(f"[Agent] Stream Delta: {delta}") 
                        
                        # 1. å¤„ç†æ–‡æœ¬å†…å®¹ (Content)
                        content = delta.get("content", "")
                        if content:
                            current_turn_text += content
                            full_response_text += content
                            yield content
                        
                        # 2. å¤„ç†å·¥å…·è°ƒç”¨ç‰‡æ®µ (Tool Calls Delta)
                        if "tool_calls" in delta:
                            has_tool_calls_in_this_turn = True
                            for tc_delta in delta["tool_calls"]:
                                idx = tc_delta.get("index", 0)
                                while len(collected_tool_calls) <= idx:
                                    collected_tool_calls.append({
                                        "id": "", 
                                        "type": "function", 
                                        "function": {"name": "", "arguments": ""}
                                    })
                                
                                target = collected_tool_calls[idx]
                                if "id" in tc_delta: target["id"] = tc_delta["id"]
                                if "function" in tc_delta:
                                    fn_delta = tc_delta["function"]
                                    if "name" in fn_delta: target["function"]["name"] = fn_delta["name"]
                                    if "arguments" in fn_delta: target["function"]["arguments"] += fn_delta["arguments"]
                
                # Debug Log: After stream
                if has_tool_calls_in_this_turn:
                    print(f"[Agent] Collected Tool Calls: {json.dumps(collected_tool_calls, ensure_ascii=False)}")
                else:
                    print(f"[Agent] No tool calls detected in Turn {turn_count}")

                # Apply Postprocessor Pipeline
                processed_stream = self.postprocessor_manager.process_stream(
                    raw_stream_source(),
                    context={"source": source, "session_id": session_id}
                )

                async for content in processed_stream:
                    yield content

                # Check if turn is finished
                if not has_tool_calls_in_this_turn:
                    # No tools called, this is the final answer
                    
                    # === NIT Dispatcher Integration (Unified Flow) ===
                    # æ£€æŸ¥æ˜¯å¦æœ‰ NIT è°ƒç”¨æŒ‡ä»¤ï¼Œå¦‚æœæœ‰åˆ™æ‰§è¡Œå¹¶è¿›å…¥ä¸‹ä¸€è½®
                    if full_response_text and full_response_text.strip():
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°è¯•æ‰§è¡Œ NITï¼Œå¦‚æœæœ‰ç»“æœï¼Œè¯´æ˜æ¨¡å‹è¯•å›¾è°ƒç”¨å·¥å…·
                        nit_results = await self._save_parsed_metadata(full_response_text, source, mcp_clients, execute_nit=True, expected_nit_id=current_nit_id)
                        
                        if nit_results:
                            print(f"[Agent] Detected {len(nit_results)} NIT calls. Continuing conversation loop.")
                            
                            # 1. å°†å½“å‰å›å¤ï¼ˆåŒ…å« NIT æŒ‡ä»¤ï¼‰è¿½åŠ åˆ°å†å²
                            # [Safety] Truncate extremely long responses to prevent context window explosion
                            safe_response_text = full_response_text
                            if len(safe_response_text) > 50000:
                                safe_response_text = safe_response_text[:50000] + "\n...(truncated by system for safety)"
                                print(f"âš ï¸ [Agent] Response truncated from {len(full_response_text)} to 50000 chars.")

                            final_messages.append({
                                "role": "assistant",
                                "content": safe_response_text
                            })
                            
                            # 2. æ„é€  Observation æ¶ˆæ¯
                            obs_text = "ã€ç³»ç»Ÿé€šçŸ¥ï¼šNITå·¥å…·æ‰§è¡Œåé¦ˆã€‘\n"
                            has_screenshot_request = False
                            should_terminate_nit_loop = False
                            
                            for res in nit_results:
                                icon = "âœ…" if res['status'] == 'success' else "âŒ"
                                out_str = str(res['output'])
                                if len(out_str) > 2000: out_str = out_str[:2000] + "...(truncated)"
                                obs_text += f"{icon} å·¥å…· [{res['plugin']}] æ‰§è¡Œå®Œæˆã€‚\nç»“æœ:\n{out_str}\n\n"
                                
                                if res['plugin'] == 'finish_task':
                                    should_terminate_nit_loop = True

                                # å®½æ¾åŒ¹é…æˆªå›¾è¯·æ±‚ (å¤„ç†å¤§å°å†™å’Œåˆ«å)
                                plugin_name_lower = res['plugin'].lower()
                                # æ”¯æŒ 'screenshot', 'see_screen' ç­‰å…³é”®è¯
                                if ('screenshot' in plugin_name_lower or 'see_screen' in plugin_name_lower) and res['status'] == 'success':
                                    has_screenshot_request = True
                                    print(f"[Agent] Detected screenshot request in NIT: {res['plugin']}")
                            
                            # 3. æ ¹æ®æ˜¯å¦æœ‰å¤šæ¨¡æ€éœ€æ±‚æ„é€ æ¶ˆæ¯å†…å®¹
                            enable_vision = config.get("enable_vision", False)
                            message_content = [{"type": "text", "text": obs_text}]
                            
                            if has_screenshot_request and enable_vision:
                                try:
                                    print("[Agent] Injecting screenshot for NIT call...")
                                    from services.screenshot_service import screenshot_manager
                                    # å¼ºåˆ¶æ•è·æœ€æ–°æˆªå›¾
                                    screenshot_data = screenshot_manager.capture()
                                    if screenshot_data:
                                        # æ³¨å…¥å›¾ç‰‡
                                        message_content.append({
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/png;base64,{screenshot_data['base64']}"
                                            }
                                        })
                                        # æ›´æ–°æ–‡æœ¬æç¤ºï¼Œå¸¦ä¸Šæ—¶é—´æˆ³ä»¥åŒºåˆ†æ—§å›¾
                                        capture_time = screenshot_data.get('time_str', datetime.now().strftime("%H:%M:%S"))
                                        obs_text += f"\n[ç³»ç»Ÿ] å·²é™„å¸¦æœ€æ–°å±å¹•æˆªå›¾ (Time: {capture_time})ã€‚"
                                        message_content[0]["text"] = obs_text
                                        print(f"[Agent] Screenshot injected successfully. Time: {capture_time}")
                                    else:
                                        print("[Agent] Screenshot capture returned None.")
                                        obs_text += "\n[ç³»ç»Ÿ] å°è¯•æˆªå›¾å¤±è´¥ï¼šæ— æ³•è·å–å›¾åƒæ•°æ®ã€‚"
                                        message_content[0]["text"] = obs_text
                                except Exception as e:
                                    print(f"[Agent] Failed to inject screenshot for NIT: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    obs_text += f"\n[ç³»ç»Ÿ] å°è¯•æˆªå›¾å¤±è´¥: {e}"
                                    message_content[0]["text"] = obs_text

                            final_messages.append({
                                "role": "user",
                                "content": message_content
                            })
                            
                            # 4. æ£€æŸ¥è¿ç»­é”™è¯¯ç†”æ–­
                            has_error = any(res['status'] == 'error' for res in nit_results)
                            if has_error:
                                consecutive_error_count += 1
                                print(f"[Agent] NIT Tool error detected. Consecutive count: {consecutive_error_count}")
                            else:
                                consecutive_error_count = 0

                            if consecutive_error_count >= 3:
                                print(f"âš ï¸ [Agent] Consecutive errors ({consecutive_error_count}) reached limit via NIT. Forcing stop.")
                                final_messages.append({
                                    "role": "system",
                                    "content": "ã€ç³»ç»Ÿç´§æ€¥å¹²é¢„ã€‘ç›‘æµ‹åˆ°ä½ å·²ç»è¿ç»­æ“ä½œå¤±è´¥ 3 æ¬¡ã€‚è¯·ç«‹å³åœæ­¢ä»»ä½•åç»­çš„æ€è€ƒä¸å·¥å…·è°ƒç”¨ï¼Œæ”¾å¼ƒå½“å‰ä»»åŠ¡ï¼Œå¹¶ä¸»åŠ¨å‘ä¸»äººæ±‡æŠ¥å¤±è´¥åŸå› ã€‚"
                                })
                                # ç¦ç”¨åç»­å·¥å…·è°ƒç”¨
                                tools_to_pass = None

                            # 4.1 è§¦å‘åæ€ (å¦‚æœå‡ºé”™)
                            if has_error:
                                print(f"âš ï¸ [Agent] NIT Tool execution error detected, triggering reflection...")
                                history_context = "\n".join([f"{m['role']}: {str(m.get('content',''))[:200]}" for m in final_messages[-5:]])
                                # å°è¯•è·å–æœ€æ–°æˆªå›¾ä¾›åæ€ä½¿ç”¨
                                latest_screenshot = None
                                try:
                                    from services.screenshot_service import screenshot_manager
                                    shot = screenshot_manager.capture()
                                    if shot: latest_screenshot = shot["base64"]
                                except: pass
                                
                                reflection_advice = await self._run_reflection(user_message, history_context, latest_screenshot)
                                
                                if reflection_advice and "NORMAL" not in reflection_advice:
                                    final_messages.append({
                                        "role": "system",
                                        "content": f"[åæ€åŠ©æ‰‹æç¤º]: æ£€æµ‹åˆ°ä¸Šä¸€æ­¥æ“ä½œå¯èƒ½å­˜åœ¨é—®é¢˜ã€‚å»ºè®®å‚è€ƒï¼š{reflection_advice}"
                                    })

                            # 5. é‡ç½®çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€è½®
                            accumulated_full_response += full_response_text + "\n"
                            full_response_text = ""
                            current_turn_text = "" 
                            collected_tool_calls = []
                            has_tool_calls_in_this_turn = False
                            
                            if should_terminate_nit_loop:
                                print("[Agent] NIT loop terminated by finish_task.")
                                break

                            # 4. ç»§ç»­å¾ªç¯
                            continue

                    if turn_count == 1 and not full_response_text.strip():
                         print("[Agent] Stream finished without any deltas.")
                         self._log_to_file("Stream finished without any deltas.")
                         err_msg = "âš ï¸ AI æ²¡æœ‰è¿”å›æœ‰æ•ˆå†…å®¹ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– API Key é…ç½®ã€‚"
                         full_response_text = err_msg
                         yield err_msg
                    break
                
                # --- Tool Execution Phase ---
                # 1. Append Assistant Message (Thought + Tool Calls) to history
                # [Safety] Truncate extremely long thought process
                safe_turn_text = current_turn_text if current_turn_text else None
                if safe_turn_text and len(safe_turn_text) > 50000:
                    safe_turn_text = safe_turn_text[:50000] + "\n...(truncated by system for safety)"

                assistant_msg = {
                    "role": "assistant",
                    "content": safe_turn_text,
                    "tool_calls": collected_tool_calls
                }
                final_messages.append(assistant_msg)
                
                # 2. Execute Tools
                intercepted_ui_data = {} # å­˜å‚¨ tool_name -> raw_data
                should_terminate_loop = False

                for tool_call in collected_tool_calls:
                    function_name = tool_call["function"]["name"]
                    args_str = tool_call["function"]["arguments"] or "{}"
                    arg_parsing_error = None
                    try:
                        function_args = json.loads(args_str)
                    except json.JSONDecodeError as e:
                        # å°è¯•å¤„ç† "Extra data" (ä¾‹å¦‚æ¨¡å‹è¾“å‡ºäº†å¤šä¸ª JSON å¯¹è±¡)
                        try:
                            function_args, _ = json.JSONDecoder().raw_decode(args_str)
                            print(f"[Agent] Recovered from Extra data error. Parsed: {function_args}")
                        except Exception:
                            print(f"[Agent] Failed to parse tool arguments: {args_str}, error: {e}")
                            arg_parsing_error = f"Failed to parse arguments: {str(e)}"
                            function_args = {}
                    except Exception as e:
                        print(f"[Agent] Failed to parse tool arguments: {args_str}, error: {e}")
                        arg_parsing_error = f"Failed to parse arguments: {str(e)}"
                        function_args = {}
                    
                    # å¦‚æœå‚æ•°è§£æå¤±è´¥ï¼Œç›´æ¥ç”Ÿæˆé”™è¯¯å“åº”ï¼Œä¸æ‰§è¡Œå‡½æ•°
                    if arg_parsing_error:
                         final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": f"Error: {arg_parsing_error}. Please ensure arguments are valid JSON.",
                        })
                         continue
                    
                    # --- Tool Execution Strategy ---
                    # 1. Security Gate: ç¡¬æ‹¦æˆªæœºåˆ¶ (Hard Isolation)
                    # å³ä½¿æ¨¡å‹â€œçŒœâ€åˆ°äº†å·¥å…·åï¼Œæˆ–è€…é€šè¿‡æ¶æ„è„šæœ¬æ³¨å…¥ï¼Œåªè¦æ¥æºæ˜¯æ‰‹æœºï¼Œå°±ç¦æ­¢æ‰§è¡Œæ•æ„Ÿå·¥å…·
                    sensitive_tool_keywords = ["screenshot", "screen", "windows", "shell", "cmd", "file", "app", "browser", "exec", "write"]
                    if source == "mobile" and any(kw in function_name.lower() for kw in sensitive_tool_keywords):
                        print(f"[ğŸ›¡ï¸ Hard Security] Blocked execution of sensitive tool '{function_name}' from mobile source.")
                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": f"Error: Permission Denied. Tool '{function_name}' is restricted for remote/mobile connections for security reasons.",
                        })
                        continue

                    # 2. Interceptors: Handle tools with special UI/Context requirements first
                    # 3. NIT Dispatcher: Unified execution for all other plugins
                    
                    if function_name == "finish_task":
                        print(f"[Agent] finish_task called. Status: {function_args.get('status', 'success')}")
                        summary = function_args.get("summary", "")
                        if summary:
                            full_response_text += summary
                            yield summary
                        
                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": "Task finished. Terminating loop.",
                        })
                        should_terminate_loop = True
                        break

                    if function_name == "search_files":
                        print(f"[Agent] Intercepting {function_name} call for UI injection...")
                        if on_status: await on_status("thinking", "æ­£åœ¨å¤„ç†å¤§æ•°æ®é‡ä»»åŠ¡...")
                        
                        try:
                            # Use legacy mapping or NIT dispatcher to get raw data
                            # Here we use legacy mapping for safety as it returns raw list/dict
                            func = TOOLS_MAPPING[function_name]
                            if asyncio.iscoroutinefunction(func):
                                raw_data = await func(**function_args)
                            else:
                                raw_data = func(**function_args)
                            
                            tag_name = "FILE_RESULTS" 
                            intercepted_ui_data[tag_name] = raw_data
                            
                            # è¾…åŠ©æ¨¡å‹åˆ†æ
                            aux_analysis = None
                            try:
                                data_list = json.loads(raw_data)
                                if isinstance(data_list, list) and len(data_list) > 0:
                                    if on_status: await on_status("thinking", "æ­£åœ¨åˆ†ææœç´¢ç»“æœ...")
                                    aux_analysis = await self._analyze_file_results_with_aux(user_message, data_list)
                            except Exception as e:
                                print(f"[Agent] Failed to trigger aux analysis: {e}")

                            try:
                                data_list = json.loads(raw_data)
                                count = len(data_list) if isinstance(data_list, list) else 1
                            except: count = "è‹¥å¹²"
                            
                            aux_msg = ""
                            if aux_analysis:
                                aux_msg = f"\n\n[è¾…åŠ©æ¨¡å‹åˆ†æç»“æœ]:\n{aux_analysis}"
                            
                            function_response = f"System: å·²æˆåŠŸå¤„ç†ã€‚è·å–åˆ° {count} æ¡æ•°æ®ï¼ŒUI åˆ—è¡¨å·²åœ¨åå°å‡†å¤‡å°±ç»ªã€‚{aux_msg}\nè¯·ç»“åˆè¾…åŠ©æ¨¡å‹çš„åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå‘ŠçŸ¥ç”¨æˆ·ä½ å·²ç»å¤„ç†å®Œæˆï¼Œå¹¶å¯ä»¥ç®€è¦å¤è¿°åˆ†æç»“è®ºã€‚"
                            print(f"[Agent] {function_name} intercepted. {count} items hidden from LLM context.")
                        except Exception as e:
                            function_response = f"Error during intercepted tool execution: {e}"

                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": function_response,
                        })
                        continue

                    elif function_name == "take_screenshot" or function_name == "see_screen":
                        print(f"[Agent] Calling tool: {function_name}")
                        
                        # æ ¹æ®å¤šæ¨¡æ€çŠ¶æ€å†³å®šæ‰§è¡Œé€»è¾‘
                        enable_vision = config.get("enable_vision", False)
                        
                        if not enable_vision:
                            # éå¤šæ¨¡æ€æ¨¡å¼ï¼šå°è¯•è°ƒç”¨ MCP è§†è§‰åˆ†æ
                            if on_status: await on_status("thinking", "æ­£åœ¨é€šè¿‡ MCP åˆ†æå±å¹•å†…å®¹...")
                            vision_description = await self._analyze_screen_with_mcp()
                            
                            function_response = f"[è§†è§‰åˆ†ææŠ¥å‘Š]:\n{vision_description}"
                            print(f"[Agent] Vision analysis via MCP complete.")
                        else:
                            # å¤šæ¨¡æ€æ¨¡å¼ï¼šç›´æ¥æ³¨å…¥æˆªå›¾
                            if on_status: await on_status("thinking", "æ­£åœ¨æŸ¥çœ‹æˆªå›¾æ± ...")
                            try:
                                from services.screenshot_service import screenshot_manager
                                
                                # 1. è·å–è¯·æ±‚çš„æ•°é‡
                                count = function_args.get("count", 1)
                                if not isinstance(count, int): count = 1
                                count = max(1, min(10, count))

                                # 2. æ•è·æœ€æ–°æˆªå›¾
                                # å¼ºåˆ¶æ•è·ä¸€å¼ æœ€æ–°çš„ï¼Œç¡®ä¿â€œæ‰€è§å³æ‰€å¾—â€ï¼Œé¿å…è¯»å–ç¼“å­˜æ± ä¸­çš„æ—§å›¾
                                latest_shot = screenshot_manager.capture()
                                
                                final_screenshots = []
                                
                                if count == 1:
                                    # å¦‚æœåªéœ€è¦ä¸€å¼ ï¼Œç›´æ¥ä½¿ç”¨åˆšåˆšæ•è·çš„è¿™å¼ ï¼Œç¡®ä¿æœ€æ–°
                                    if latest_shot:
                                        final_screenshots = [latest_shot]
                                else:
                                    # å¦‚æœéœ€è¦å¤šå¼ ï¼ˆå›æº¯ï¼‰ï¼Œåˆ™ä»æ± å­ä¸­å–
                                    # ä½¿ç”¨è¾ƒçŸ­çš„æœ‰æ•ˆæœŸï¼ˆå¦‚ 15 ç§’ï¼‰ï¼Œç¡®ä¿è·å–åˆ°çš„æ˜¯åˆšåˆšæˆªå–çš„
                                    recent_screenshots = screenshot_manager.get_recent(count, max_age=15)
                                    final_screenshots = recent_screenshots
                                
                                if not final_screenshots:
                                    function_response = "âŒ æ— æ³•è·å–æœ€æ–°æˆªå›¾ï¼ˆå¯èƒ½æˆªå›¾å¤±è´¥ï¼‰ã€‚"
                                else:
                                    # 3. å°†æˆªå›¾æ³¨å…¥åˆ°ä¸‹ä¸€è½®çš„ä¸Šä¸‹æ–‡ä¸­
                                    content = [{"type": "text", "text": f"ç³»ç»Ÿæç¤ºï¼šä»¥ä¸‹æ˜¯æœ€è¿‘æ•è·çš„ {len(final_screenshots)} å¼ å±å¹•æˆªå›¾ï¼ˆæŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰ï¼š"}]
                                    
                                    for i, shot in enumerate(final_screenshots):
                                        content.append({
                                            "type": "text", 
                                            "text": f"--- æˆªå›¾ {i+1} (æ•è·æ—¶é—´: {shot['time_str']}) ---"
                                        })
                                        content.append({
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/png;base64,{shot['base64']}"
                                            }
                                        })
                                    
                                    screenshot_msg = {
                                        "role": "user",
                                        "content": content
                                    }
                                    final_messages.append(screenshot_msg)
                                    print(f"[Agent] {len(final_screenshots)} screenshots injected into context. (Newest: {final_screenshots[-1]['time_str']})")
                                    
                                    function_response = f"å·²æˆåŠŸè·å–å¹¶å‘é€äº†æœ€è¿‘çš„ {len(final_screenshots)} å¼ æˆªå›¾ã€‚è¯·æŸ¥çœ‹æœ€æ–°çš„æ¶ˆæ¯ä¸­çš„å›¾ç‰‡è¿›è¡Œåˆ†æã€‚"
                            except Exception as e:
                                 function_response = f"æˆªå›¾å·¥å…·æ‰§è¡Œå‡ºé”™: {e}"

                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": function_response,
                        })
                        continue

                    # --- NIT Dispatcher Integration ---
                    from nit_core.dispatcher import get_dispatcher
                    nit_dispatcher = get_dispatcher()
                    
                    # å½’ä¸€åŒ–å·¥å…·å
                    normalized_name = nit_dispatcher.parser.normalize_key(function_name)
                    
                    # ä¿¡ä»» Dispatcher çš„æ³¨å†Œè¡¨
                    if normalized_name in nit_dispatcher.list_plugins():
                        print(f"[Agent] Delegating tool {function_name} to NITDispatcher (Unified Flow)...")
                        if on_status: await on_status("thinking", f"æ­£åœ¨è°ƒç”¨èƒ½åŠ›: {function_name}...")
                        
                        try:
                            # NIT æ’ä»¶ç»Ÿä¸€æ¥å£ï¼šæ¥æ”¶ params å­—å…¸
                            result = await nit_dispatcher._execute_plugin(function_name, function_args)
                            
                            # å¦‚æœç»“æœæ˜¯å¤æ‚å¯¹è±¡ï¼ŒDispatcher é‡Œçš„æ’ä»¶åº”è¯¥å·²ç»å¤„ç†æˆäº†å­—ç¬¦ä¸²æˆ–ç‰¹å®šç»“æ„
                            # è¿™é‡Œæˆ‘ä»¬åªè´Ÿè´£è½¬ä¸ºå­—ç¬¦ä¸²å›ä¼ ç»™ LLM
                            function_response = str(result)
                            print(f"[Agent] NIT tool {function_name} executed successfully.")
                            
                            # [Feature] å®æ—¶çŠ¶æ€åŒæ­¥
                            # å¦‚æœæ˜¯ update_character_statusï¼Œè§£æå…¶è¿”å›çš„ triggers å¹¶ç«‹å³æ¨é€åˆ°å‰ç«¯
                            if function_name in ["update_character_status", "update_status", "set_status"]:
                                try:
                                    import json
                                    triggers = json.loads(str(result))
                                    
                                    # 1. æ„é€  SSE æ ¼å¼çš„ JSON æ•°æ®
                                    sse_payload = json.dumps({"triggers": triggers}, ensure_ascii=False)
                                    sse_message = f"data: {sse_payload}\n\n"
                                    
                                    # 2. æ¨é€åˆ°å‰ç«¯ (é€šè¿‡ yield SSE)
                                    yield sse_message
                                    
                                    # 3. å°è¯•å¹¿æ’­ç»™ VoiceManager (åŒä¿é™©ï¼Œé€‚ç”¨äºè¯­éŸ³æ¨¡å¼)
                                    try:
                                        from services.voice_manager import voice_manager
                                        await voice_manager.broadcast({"type": "triggers", "data": triggers})
                                    except:
                                        pass
                                        
                                    print(f"[Agent] Status update pushed to frontend: {sse_payload[:50]}...")
                                except Exception as e:
                                    print(f"[Agent] Failed to push status update: {e}")

                            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯ search_filesï¼Œä¸”è¿”å›ç»“æœå¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦æˆªæ–­æˆ–ç”±è¾…åŠ©æ¨¡å‹å¤„ç†
                            # æ€è·¯æ˜¯æ’ä»¶å†…éƒ¨è‡ªå·±å¤„ç†å¥½è¿”å›å†…å®¹
                            # è¿™é‡Œä¿ç•™ä¸€ä¸ªç®€å•çš„æˆªæ–­ä¿æŠ¤
                            if len(function_response) > 10000:
                                function_response = function_response[:10000] + "\n... (result truncated)"

                        except Exception as e:
                            print(f"[Agent] NIT tool {function_name} failed: {e}")
                            function_response = f"Error executing tool: {e}"
                            
                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": function_response,
                        })
                        continue
                    
                    # --- MCP Tool Handling ---
                    if function_name.startswith("mcp_") and mcp_tool_map:
                        real_tool_name = function_name[4:]
                        client = mcp_tool_map.get(function_name)
                        if not client:
                            print(f"[Agent] MCP tool {function_name} not found in map")
                            mcp_response = f"Error: MCP tool {function_name} not found."
                        else:
                            print(f"[Agent] Calling MCP tool: {real_tool_name} on {client.name}")
                            if on_status: await on_status("thinking", f"æ­£åœ¨è°ƒç”¨æ’ä»¶ ({client.name}): {real_tool_name}...")
                            
                            import time
                            start_time = time.time()
                            try:
                                mcp_response = await client.call_tool(real_tool_name, function_args)
                                duration = time.time() - start_time
                                print(f"[Agent] MCP tool {real_tool_name} executed in {duration:.2f}s")
                            except Exception as e:
                                print(f"[Agent] MCP tool {real_tool_name} failed: {e}")
                                mcp_response = f"Error: {e}"

                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": str(mcp_response),
                        })

                    else:
                        # --- Fallback for Unknown Tools ---
                        print(f"[Agent] Tool {function_name} not found in NIT Registry or MCP.")
                        final_messages.append({
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": function_name,
                            "content": f"Error: Tool '{function_name}' not found or not supported.",
                        })

                # 3. è§¦å‘æŒ‰éœ€åæ€æœºåˆ¶
                last_tool_response = final_messages[-1].get("content", "")
                is_tool_error = "error" in str(last_tool_response).lower() or "fail" in str(last_tool_response).lower()
                
                if is_tool_error:
                    consecutive_error_count += 1
                    print(f"[Agent] Tool error detected. Consecutive count: {consecutive_error_count}")
                else:
                    consecutive_error_count = 0

                # [Feature] è¿ç»­é”™è¯¯ 3 æ¬¡ç†”æ–­æœºåˆ¶
                if consecutive_error_count >= 3:
                    print(f"âš ï¸ [Agent] Consecutive errors ({consecutive_error_count}) reached limit. Forcing stop.")
                    final_messages.append({
                        "role": "system",
                        "content": "ã€ç³»ç»Ÿç´§æ€¥å¹²é¢„ã€‘ç›‘æµ‹åˆ°ä½ å·²ç»è¿ç»­æ“ä½œå¤±è´¥ 3 æ¬¡ã€‚è¯·ç«‹å³åœæ­¢ä»»ä½•åç»­çš„æ€è€ƒä¸å·¥å…·è°ƒç”¨ï¼Œæ”¾å¼ƒå½“å‰ä»»åŠ¡ï¼Œå¹¶ä¸»åŠ¨å‘ä¸»äººæ±‡æŠ¥å¤±è´¥åŸå› ã€‚"
                    })
                    # ç¦ç”¨å·¥å…·ï¼Œå¼ºåˆ¶ LLM åªèƒ½å›å¤æ–‡æœ¬
                    tools_to_pass = None

                if is_tool_error:
                    print(f"âš ï¸ [Agent] Tool execution error detected, triggering reflection...")
                    history_context = "\n".join([f"{m['role']}: {str(m.get('content',''))[:200]}" for m in final_messages[-5:]])
                    # å°è¯•è·å–æœ€æ–°æˆªå›¾ä¾›åæ€ä½¿ç”¨
                    latest_screenshot = None
                    try:
                        from services.screenshot_service import screenshot_manager
                        shot = screenshot_manager.capture()
                        if shot: latest_screenshot = shot["base64"]
                    except: pass
                    
                    reflection_advice = await self._run_reflection(user_message, history_context, latest_screenshot)
                    
                    if reflection_advice and "NORMAL" not in reflection_advice:
                        final_messages.append({
                            "role": "system",
                            "content": f"[åæ€åŠ©æ‰‹æç¤º]: æ£€æµ‹åˆ°ä¸Šä¸€æ­¥æ“ä½œå¯èƒ½å­˜åœ¨é—®é¢˜ã€‚å»ºè®®å‚è€ƒï¼š{reflection_advice}"
                        })

                # 4. Yield UI Tags Immediately
                if intercepted_ui_data:
                    for tag_name, raw_json in intercepted_ui_data.items():
                        tag = f"\n<{tag_name}>{raw_json}</{tag_name}>"
                        full_response_text += tag
                        yield tag
                        print(f"[Agent] Appended hidden {tag_name} tag to response.")
                
                if should_terminate_loop:
                    print("[Agent] Loop terminated by finish_task.")
                    break
                
                # Loop continues to next turn...

            try:
                # 5. æœ€ç»ˆåˆå¹¶æ‰€æœ‰è½®æ¬¡çš„å†…å®¹ç”¨äºæŒä¹…åŒ–
                full_response_text = accumulated_full_response + full_response_text
                
                # Capture raw text before post-processing
                raw_full_text = full_response_text

                # Post-process full response text (Batch mode) before saving
                # This ensures memory and downstream services get clean text without protocol markers
                if full_response_text:
                    try:
                        full_response_text = await self.postprocessor_manager.process(
                            full_response_text, 
                            context={"source": source, "session_id": session_id}
                        )
                    except Exception as pp_e:
                        print(f"[Agent] Postprocessor failed: {pp_e}. Using raw text.")

                # ä»…åœ¨æ­£å¸¸ç”Ÿæˆå›å¤ï¼ˆä¸”ä¸æ˜¯æŠ¥é”™ï¼‰æ—¶æ‰ä¿å­˜å¯¹è¯è®°å½•
                # ç”¨æˆ·æ¶ˆæ¯ä¸ Pero å›å¤è¿›è¡ŒåŸå­æ€§ç»‘å®šä¿å­˜
                
                # [Robustness] Fallback extraction for user_message if missing
                if not user_message:
                    # Priority 1: Check override (Voice Mode)
                    if user_text_override:
                        user_message = user_text_override
                        print(f"[Agent] User message restored from override: '{user_message[:20]}...'")
                    else:
                        # Priority 2: Search in messages
                        print(f"[Agent] User message missing. Searching in {len(messages)} input messages...")
                        for m in reversed(messages):
                            if m.get("role") == "user":
                                content = m.get("content", "")
                                if isinstance(content, str):
                                    user_message = content
                                elif isinstance(content, list):
                                    texts = [item["text"] for item in content if item.get("type") == "text"]
                                    user_message = " ".join(texts)
                                break
                        if user_message:
                            print(f"[Agent] Fallback extracted user message: '{user_message[:20]}...'")
                        else:
                            print(f"[Agent] CRITICAL: Failed to extract user message from input. Logs will NOT be saved.")

                should_save = not skip_save and user_message and full_response_text
                print(f"[Agent] Log Save Check: save={should_save} (skip_save={skip_save}, has_user_msg={bool(user_message)}, resp_len={len(full_response_text) if full_response_text else 0})")
                
                if should_save:
                    # å¦‚æœæœ‰è¦†ç›–æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨è¦†ç›–æ–‡æœ¬ï¼ˆç¡®ä¿éŸ³é¢‘è¾“å…¥æ—¶ä¹Ÿèƒ½å­˜ä¸‹æ–‡æœ¬ï¼‰
                    final_user_msg = user_text_override if user_text_override else user_message
                    try:
                        await self.memory_service.save_log_pair(
                            self.session, 
                            source, 
                            session_id, 
                            final_user_msg, 
                            full_response_text, 
                            pair_id,
                            assistant_raw_content=raw_full_text
                        )
                        print(f"[Agent] Conversation log pair saved (pair_id: {pair_id})")
                    except Exception as e:
                         print(f"[Agent] Failed to save log pair: {e}")
                else:
                     if not skip_save:
                         print(f"[Agent] Skipping save. Reason: user_msg={bool(user_message)}, resp_valid={bool(full_response_text and not full_response_text.startswith('Error:'))}")
                
                if full_response_text:
                    await self._save_parsed_metadata(full_response_text, source, mcp_clients if 'mcp_clients' in locals() else None, execute_nit=False)
                
                # è§¦å‘ Scorer æœåŠ¡è¿›è¡Œè®°å¿†æå– (èŒè´£åˆ†ç¦» - åå°å¼‚æ­¥æ‰§è¡Œ)
                # [Optimization] Do not run scorer if the response is an error
                is_error_response = (
                    full_response_text.startswith("Error:") or 
                    full_response_text.startswith("Network Error") or
                    full_response_text.startswith("âš ï¸")
                )

                if not skip_save and user_message and full_response_text and not is_error_response:
                    final_user_msg = user_text_override if user_text_override else user_message
                    if len(full_response_text) > 5:
                        # ä½¿ç”¨ background_task åŒ…è£…ä»¥ç¡®ä¿ç‹¬ç«‹ Session
                        asyncio.create_task(self._run_scorer_background(final_user_msg, full_response_text, source, pair_id=pair_id))

                # æ˜¾å¼æäº¤ï¼Œç¡®ä¿åœ¨æµå¼å“åº”çš„ä¸Šä¸‹æ–‡ä¸­æ•°æ®å·²æŒä¹…åŒ–
                await self.session.commit()
                
                # [Trigger Dream] 3% probability to trigger background memory consolidation
                import random
                if random.random() < 0.03:
                     asyncio.create_task(self._trigger_dream())

            except Exception as log_err:
                print(f"Failed to save conversation log (success path): {log_err}")

        except Exception as e:
            import traceback
            error_msg = f"Error: {str(e)}"
            print(f"Agent Chat Error (Inner): {traceback.format_exc()}")
            # [Troubleshooting] Attempt to save log even on error (User request: logs missing)
            try:
                # Ensure user_message is available
                final_u_msg = user_message
                if not final_u_msg and user_text_override:
                    final_u_msg = user_text_override
                
                # Append error to response so it's recorded
                final_response = full_response_text + f"\n\n[System Error]: {error_msg}"
                
                if final_u_msg:
                     await self.memory_service.save_log_pair(
                        self.session, 
                        source, 
                        session_id, 
                        final_u_msg, 
                        final_response, 
                        pair_id
                    )
                     print(f"[Agent] Error log saved (pair_id: {pair_id})")
            except Exception as save_err:
                print(f"[Agent] Failed to save error log: {save_err}")

            yield error_msg
        finally:
            if session_id:
                task_manager.unregister(session_id)

            # æœ€åçš„å…œåº•å¤„ç†ï¼Œæ¸…ç† MCP å®¢æˆ·ç«¯èµ„æº
            if 'mcp_clients' in locals():
                for client in mcp_clients:
                    try:
                        await client.close()
                    except:
                        pass
            pass
