# Copyright (c) 2026 YoKONCy. All rights reserved.
# This component (Memory Service) is protected under GNU GPL-3.0.
# Any use in proprietary/closed-source software is strictly prohibited.
# Fingerprint: PERO_CORE_MEM_SYS_v0.1_YK

from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlmodel import select, delete, desc, and_
from sqlmodel.ext.asyncio.session import AsyncSession
from models import Memory, ConversationLog, MemoryRelation
import re
import json

# [Global State] é«˜æ€§èƒ½ Rust å¼•æ“å•ä¾‹
# PEDSA (Parallel Energy-Decay Spreading Activation) ç®—æ³•æ ¸å¿ƒå®ç°
# -------------------------------------------------------------------------
# å·¥ç¨‹è¯´æ˜ï¼š
# ä¸ºä»€ä¹ˆä¸ç”¨æ ‡å‡†çš„å›¾æ•°æ®åº“ï¼ˆå¦‚ Neo4jï¼‰ï¼Ÿ
# 1. å»¶è¿Ÿï¼šNeo4j çš„ Cypher æŸ¥è¯¢åœ¨å¤„ç†è¿™ç§â€œæ— é™æ‰©æ•£â€æ—¶ä¼šäº§ç”Ÿå¤§é‡éšæœº IOï¼Œå¯¼è‡´ 10 æ­¥ä»¥ä¸Šçš„æ‰©æ•£å»¶è¿Ÿè¶…è¿‡ 500msã€‚
# 2. å†…å­˜ï¼šæˆ‘ä»¬éœ€è¦åœ¨è¾¹ç¼˜ä¾§ï¼ˆç”¨æˆ· PCï¼‰è¿è¡Œã€‚é€šè¿‡ Rust å®ç°çš„ç±» CSR (Simulated CSR) ç¨€ç–çŸ©é˜µï¼Œæˆ‘ä»¬å°† 100 äº¿ä¸ªå…³è”çš„å†…å­˜å ç”¨å‹åˆ°äº† 2GB ä»¥å†…ã€‚
# 3. å®æ—¶æ€§ï¼šPEDSA éœ€è¦åœ¨æ¯ä¸€å¸§è§†è§‰è¾“å…¥æ—¶è¿›è¡Œèƒ½é‡æ›´æ–°ï¼Œè¿™æ˜¯ä¼ ç»Ÿäº‹åŠ¡æ•°æ®åº“æ— æ³•æ»¡è¶³çš„ååé‡ã€‚
# -------------------------------------------------------------------------
_rust_engine = None

async def get_rust_engine(session: AsyncSession):
    global _rust_engine
    if _rust_engine is not None:
        return _rust_engine
    
    try:
        from pero_memory_core import CognitiveGraphEngine
        # æŠ€æœ¯é˜²å¾¡è¯´æ˜ï¼š
        # 1. é‡‡ç”¨ç±» CSR (Simulated CSR) ç¨€ç–çŸ©é˜µå­˜å‚¨äº¿çº§å…³è”ï¼Œå†…å­˜å ç”¨æä½ã€‚
        # 2. æ‰©æ•£ç®—å­æ»¡è¶³æ”¶æ•›æ€§è¯æ˜ (è¯¦è§ benchmarks/KDN_mathematical_proof.md)ï¼Œé˜²æ­¢æ¿€æ´»çˆ†ç‚¸ã€‚
        print("[Memory] æ­£åœ¨åˆå§‹åŒ– PEDSA è®¤çŸ¥å¼•æ“ (Rust Core)...", flush=True)
        _rust_engine = CognitiveGraphEngine()
        _rust_engine.configure(max_active_nodes=10000, max_fan_out=20)
        
        # é¢„åŠ è½½æ‰€æœ‰å…³ç³» (é‡‡ç”¨åˆ†æ‰¹åŠ è½½ç­–ç•¥ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºæˆ–è¿›ç¨‹å¡æ­»)
        # é¢„åŠ è½½æ‰€æœ‰å…³ç³» (åˆ†æ‰¹åŠ è½½ä»¥é˜²æ­¢å†…å­˜æº¢å‡ºæˆ–å†»ç»“)
        # ä¼˜åŒ–è¯´æ˜ï¼š
        # 1. ä½¿ç”¨ offset/limit åˆ†é¡µè¯»å–æ•°æ®åº“ï¼Œé¿å…ä¸€æ¬¡æ€§å°†ç™¾ä¸‡çº§æ•°æ®åŠ è½½åˆ° Python å†…å­˜ã€‚
        # 2. åˆ†æ‰¹æ¬¡è°ƒç”¨ Rust å¼•æ“çš„ batch_add_connectionsï¼Œé™ä½ FFI è°ƒç”¨çš„ç¬æ—¶è´Ÿè½½ã€‚
        BATCH_SIZE = 5000  # æ¯æ‰¹æ¬¡å¤„ç† 5000 æ¡
        total_loaded = 0
        
        # 1. åˆ†æ‰¹åŠ è½½ MemoryRelation (è¯­ä¹‰å…³è”)
        mr_offset = 0
        while True:
            statement = select(MemoryRelation).offset(mr_offset).limit(BATCH_SIZE)
            relations = (await session.exec(statement)).all()
            
            if not relations:
                break
            
            # è½¬æ¢ä¸º Rust å¼•æ“éœ€è¦çš„å…ƒç»„æ ¼å¼ (source, target, strength)
            chunk_relations = [(rel.source_id, rel.target_id, rel.strength) for rel in relations]
            _rust_engine.batch_add_connections(chunk_relations)
            
            total_loaded += len(relations)
            mr_offset += BATCH_SIZE
        
        # 2. åˆ†æ‰¹åŠ è½½ Prev/Next é“¾è¡¨å…³ç³» (æ—¶é—´åºå…³è”)
        mem_offset = 0
        while True:
            statement_mem = select(Memory.id, Memory.prev_id, Memory.next_id).where(
                (Memory.prev_id != None) | (Memory.next_id != None)
            ).offset(mem_offset).limit(BATCH_SIZE)
            
            mem_links = (await session.exec(statement_mem)).all()
            
            if not mem_links:
                break
            
            chunk_links = []
            for mid, prev_id, next_id in mem_links:
                if prev_id: chunk_links.append((mid, prev_id, 0.2))
                if next_id: chunk_links.append((mid, next_id, 0.2))
            
            if chunk_links:
                _rust_engine.batch_add_connections(chunk_links)
                total_loaded += len(chunk_links)
            
            mem_offset += BATCH_SIZE
            
        print(f"[Memory] Rust å¼•æ“å·²åŠ è½½ {total_loaded} ä¸ªè¿æ¥ (åˆ†æ‰¹åŠ è½½)ã€‚", flush=True)
    except Exception as e:
        print(f"[Memory] åˆå§‹åŒ– Rust å¼•æ“å¤±è´¥: {e}")
        _rust_engine = False # æ ‡è®°ä¸ºä¸å¯ç”¨
        
    return _rust_engine

class MemoryService:
    @staticmethod
    async def save_memory(
        session: AsyncSession, 
        content: str, 
        tags: str = "", 
        clusters: str = "", # æ–°å¢ clusters å‚æ•°
        importance: int = 1, 
        base_importance: float = 1.0, 
        sentiment: str = "neutral",
        msg_timestamp: Optional[str] = None, 
        source: str = "desktop", 
        memory_type: str = "event",
        agent_id: str = "pero" # Multi-Agent Isolation
    ) -> Memory:
        from datetime import datetime
        from sqlmodel import desc
        from services.vector_service import vector_service
        from services.embedding_service import embedding_service
        
        # 1. æŸ¥æ‰¾ä¸Šä¸€æ¡è®°å¿† (The Tail of the Time-Axis)
        # å¢åŠ  agent_id è¿‡æ»¤ï¼Œç¡®ä¿åªé“¾æ¥åˆ°åŒä¸€ä¸ª Agent çš„è®°å¿†é“¾
        statement = select(Memory).where(Memory.agent_id == agent_id).order_by(desc(Memory.timestamp)).limit(1)
        last_memory_result = await session.exec(statement)
        last_memory = last_memory_result.first()

        prev_id = last_memory.id if last_memory else None

        # 2. åˆ›å»ºæ–°è®°å¿†
        # ç”Ÿæˆ Embedding (ç”¨äºå†™å…¥ VectorDB)
        # æ³¨æ„ï¼šembedding_json å­—æ®µåœ¨ SQLite ä¸­ä¿ç•™ä½œä¸ºå¤‡ä»½æˆ–å…¼å®¹ï¼Œä½†ä¸»è¦æŸ¥è¯¢èµ° VectorDB
        embedding_vec = embedding_service.encode_one(content)
        
        # [Fix] ç¡®ä¿åœ¨å†™å…¥ DB ä¹‹å‰å°è¯•è·å– embeddingï¼Œå¦‚æœå¤±è´¥åˆ™è®°å½•è­¦å‘Š
        # å³ä½¿è¿™é‡Œæ˜¯ []ï¼Œä¸‹é¢åŒæ­¥å†™å…¥ VectorDB æ—¶ä¹Ÿä¼šè¢«è·³è¿‡ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´ã€‚
        # å› æ­¤ï¼Œå¦‚æœ embedding ä¸ºç©ºï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘é‡è¯•æˆ–è®°å½•ä¸¥é‡é”™è¯¯ã€‚
        if not embedding_vec:
             print(f"[MemoryService] è­¦å‘Š: è®°å¿†å†…å®¹åµŒå…¥ç”Ÿæˆå¤±è´¥: {content[:30]}...")

        embedding_json = json.dumps(embedding_vec)

        memory = Memory(
            content=content,
            tags=tags,
            clusters=clusters, # ä¿å­˜ç°‡
            importance=importance,
            base_importance=base_importance,
            sentiment=sentiment,
            msgTimestamp=msg_timestamp,
            realTime=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            source=source,
            type=memory_type,
            prev_id=prev_id,
            next_id=None,
            embedding_json=embedding_json, # ä»ä¿ç•™ä¸€ä»½åœ¨ SQLite
            agent_id=agent_id
        )
        session.add(memory)
        await session.commit()
        await session.refresh(memory)

        # 3. åŒæ­¥å†™å…¥ VectorDB
        if embedding_vec:
            try:
                # [Feature] æ ‡ç­¾åŠ æƒå‘é‡ (Tag Weighted Embedding)
                # å°† tags é™„åŠ åˆ°å†…å®¹ä¸­è¿›è¡Œå‘é‡åŒ–ï¼Œè™½ç„¶è¿™é‡Œ vector_service.add_memory æ¥æ”¶çš„æ˜¯ embedding_vec
                # ä½†æˆ‘ä»¬åœ¨ embedding_service.encode_one(content) æ—¶ä¼ å…¥çš„åªæ˜¯ content
                # 
                # æ”¹è¿›æ–¹æ¡ˆï¼š
                # å®é™…ä¸Šï¼Œæˆ‘ä»¬åœ¨ step 2 å·²ç»ç”Ÿæˆäº† embedding_vec (åªåŸºäº content)ã€‚
                # ä¸ºäº†å¢å¼ºæ ‡ç­¾æƒé‡ï¼Œæˆ‘ä»¬åº”è¯¥ç”Ÿæˆä¸€ä¸ª "enriched_embedding" ä»…ç”¨äº VectorDB ç´¢å¼•ï¼Œ
                # è€Œ content ä¿æŒåŸæ ·ç”¨äºå±•ç¤ºã€‚
                # 
                # ä½†ä¸ºäº†ä¸ç ´åç°æœ‰é€»è¾‘ (sqlite é‡Œçš„ embedding_json ä¹Ÿæ˜¯åŸºäº content)ï¼Œ
                # æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªç­–ç•¥ï¼š
                # å¦‚æœæœ‰ tagsï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªæ··åˆæ–‡æœ¬ "tags: ... content: ..." é‡æ–°ç”Ÿæˆå‘é‡ç”¨äº VectorDBã€‚
                
                final_embedding = embedding_vec
                if tags:
                    # ç®€å•åŠ æƒç­–ç•¥ï¼šå°† tags æ”¾åœ¨å‰é¢ï¼Œé‡å¤ 2 æ¬¡ä»¥å¢åŠ æƒé‡
                    # enriched_text = f"{tags} {tags} {content}"
                    # æˆ–è€…æ›´è‡ªç„¶çš„:
                    enriched_text = f"{tags} {tags} {content}"
                    final_embedding = embedding_service.encode_one(enriched_text)
                    
                    # [Feature] TagMemo Indexing
                    # å°†æ ‡ç­¾ç‹¬ç«‹å­˜å…¥ Tag Index
                    tag_list = [t.strip() for t in tags.split(',') if t.strip()]
                    if tag_list:
                        try:
                            tag_embeddings = embedding_service.encode(tag_list)
                            for i, tag_name in enumerate(tag_list):
                                vector_service.add_tag(tag_name, tag_embeddings[i])
                        except Exception as tag_e:
                            print(f"[MemoryService] ç´¢å¼•æ ‡ç­¾å¤±è´¥: {tag_e}")
                
                # æ„å»ºå…ƒæ•°æ®
                metadata_dict = {
                    "type": memory_type,
                    "timestamp": memory.timestamp,
                    "importance": float(importance),
                    "tags": tags, 
                    "clusters": clusters,
                    "agent_id": agent_id
                }
                
                # [ç‰¹æ€§] æ€è€ƒç®¡é“ï¼šå±•å¼€ç°‡ä»¥è¿›è¡Œç²¾ç¡®è¿‡æ»¤
                # clusters="[é€»è¾‘æ¨ç†ç°‡],[åæ€ç°‡]" -> metadata={"cluster_é€»è¾‘æ¨ç†ç°‡": True, "cluster_åæ€ç°‡": True}
                if clusters:
                    cluster_list = [c.strip() for c in clusters.split(',') if c.strip()]
                    for c in cluster_list:
                        # å¦‚æœå­˜åœ¨æ‹¬å·åˆ™ç§»é™¤
                        clean_c = c.replace('[', '').replace(']', '')
                        if clean_c:
                            metadata_dict[f"cluster_{clean_c}"] = True

                vector_service.add_memory(
                    memory_id=memory.id,
                    content=content,
                    embedding=final_embedding,
                    metadata=metadata_dict
                )
            except Exception as e:
                print(f"[MemoryService] åŒæ­¥åˆ° VectorDB å¤±è´¥: {e}")
        else:
            # [Fix] å¦‚æœ embedding ä¸ºç©ºï¼Œå¼ºåˆ¶è¿›è¡Œä¸€æ¬¡åŒæ­¥é‡è¯•ï¼Œæˆ–åŠ å…¥åå°é‡è¯•é˜Ÿåˆ—
            # è¿™é‡Œç®€å•åœ°åšä¸€æ¬¡åŒæ­¥é‡è¯•ï¼ˆé˜»å¡å¼ï¼‰ï¼Œç¡®ä¿å…³é”®æ•°æ®ä¸ä¸¢å¤±
            print(f"[MemoryService] Embedding ä¸ºç©ºï¼Œæ­£åœ¨å¯¹ Memory ID {memory.id} è¿›è¡ŒåŒæ­¥é‡è¯•...")
            try:
                # å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹å¹¶ç¼–ç 
                retry_vec = embedding_service.encode_one(content)
                if retry_vec:
                    # æ›´æ–° SQL
                    memory.embedding_json = json.dumps(retry_vec)
                    session.add(memory)
                    await session.commit()
                    
                    # å†™å…¥ VectorDB
                    vector_service.add_memory(
                        memory_id=memory.id,
                        content=content,
                        embedding=retry_vec,
                        metadata={
                            "type": memory_type,
                            "timestamp": memory.timestamp,
                            "importance": float(importance),
                            "tags": tags,
                            "clusters": clusters,
                            "agent_id": agent_id
                        }
                    )
                    print(f"[MemoryService] Memory ID {memory.id} é‡è¯•æˆåŠŸã€‚")
                else:
                    print(f"[MemoryService] ä¸¥é‡é”™è¯¯: é‡è¯•å¤±è´¥ã€‚Memory {memory.id} å·²å­˜å‚¨ä½†æ— å‘é‡ç´¢å¼•ã€‚")
            except Exception as retry_e:
                print(f"[MemoryService] é‡è¯•å¼‚å¸¸: {retry_e}")

        # 4. æ›´æ–°ä¸Šä¸€æ¡è®°å¿†çš„ next_id (åŒå‘é“¾è¡¨ç»´æŠ¤)
        if last_memory:
            last_memory.next_id = memory.id
            session.add(last_memory)
            await session.commit()
            
            # [Optimization] åŒæ­¥æ›´æ–°å…¨å±€ Rust å¼•æ“å•ä¾‹
            try:
                engine = await get_rust_engine(session)
                if engine:
                    # æ·»åŠ  prev/next åŒå‘é“¾æ¥æƒé‡
                    engine.batch_add_connections([
                        (memory.id, last_memory.id, 0.2),
                        (last_memory.id, memory.id, 0.2)
                    ])
            except: pass

        return memory

    @staticmethod
    async def save_log(session: AsyncSession, source: str, session_id: str, role: str, content: str, metadata: dict = None, pair_id: str = None, raw_content: str = None, agent_id: str = "pero") -> ConversationLog:
        """ä¿å­˜åŸå§‹å¯¹è¯è®°å½•åˆ° ConversationLog"""
        # 1. ç§»é™¤ NIT åè®®æ ‡è®° (Non-invasive Integration Tools)
        from nit_core.dispatcher import remove_nit_tags
        cleaned_content = remove_nit_tags(content)

        # 2. æ¸…æ´—çœŸæ­£æ„ä¹‰ä¸Šçš„å¤§æ•°æ®é‡æŠ€æœ¯æ ‡ç­¾
        big_data_tags = ['FILE_RESULTS', 'MEMORY_LIST', 'SEARCH_RESULTS']
        for tag in big_data_tags:
            # ä»…åœ¨å†…å®¹ç¡®å®å¾ˆé•¿æ—¶æ‰æŠ˜å æŠ€æœ¯æ ‡ç­¾ï¼Œé¿å…è¯¯ä¼¤
            pattern = rf'<{tag}>([\s\S]{{1000,}}?)</{tag}>'
            cleaned_content = re.sub(pattern, f'<{tag}>[å·²æŠ˜å å¤§æ•°æ®é‡å†…å®¹]</{tag}>', cleaned_content)
        
        log = ConversationLog(
            source=source,
            session_id=session_id,
            role=role,
            content=cleaned_content,
            raw_content=raw_content, # ä¿å­˜åŸå§‹å†…å®¹
            metadata_json=json.dumps(metadata or {}),
            pair_id=pair_id,
            agent_id=agent_id
        )
        session.add(log)
        # æ³¨æ„ï¼šè¿™é‡Œå»æ‰äº† commit()ï¼Œæ”¹ä¸ºç”±å¤–éƒ¨æˆ– save_log_pair ç»Ÿä¸€æ§åˆ¶
        return log

    @staticmethod
    async def save_log_pair(session: AsyncSession, source: str, session_id: str, user_content: str, assistant_content: str, pair_id: str, metadata: dict = None, assistant_raw_content: str = None, agent_id: str = "pero", user_metadata: dict = None):
        """åŸå­æ€§ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ä¸åŠ©æ‰‹å›å¤æˆå¯¹è®°å½•"""
        try:
            # [ç‰¹æ€§] ç³»ç»Ÿè§¦å‘è§’è‰²ä¿®æ­£
            # å¦‚æœå†…å®¹ä»¥ã€ç³»ç»Ÿè§¦å‘ã€‘å¼€å¤´ï¼Œåˆ™å°†è§’è‰²ä¿®æ­£ä¸º system
            user_role = "user"
            if user_content and user_content.startswith("ã€ç³»ç»Ÿè§¦å‘ã€‘"):
                user_role = "system"

            # Use user_metadata if provided, else fall back to metadata (shared)
            u_meta = user_metadata if user_metadata is not None else metadata

            # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯è®°å½•
            user_log = await MemoryService.save_log(session, source, session_id, user_role, user_content, u_meta, pair_id, agent_id=agent_id)
            # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯è®°å½•
            # ä¸ºåŠ©æ‰‹ä¼ é€’åŸå§‹å†…å®¹
            assistant_log = await MemoryService.save_log(session, source, session_id, "assistant", assistant_content, metadata, pair_id, raw_content=assistant_raw_content, agent_id=agent_id)
            
            await session.commit()
            await session.refresh(user_log)
            await session.refresh(assistant_log)

            # [Feature] Broadcast new messages to Gateway (Event-Driven)
            try:
                from services.gateway_client import gateway_client
                from proto import perolink_pb2
                import uuid
                import time

                async def broadcast_log(log):
                    envelope = perolink_pb2.Envelope()
                    envelope.id = str(uuid.uuid4())
                    envelope.source_id = "memory_service"
                    envelope.target_id = "broadcast"
                    envelope.timestamp = int(time.time() * 1000)
                    
                    envelope.request.action_name = "new_message"
                    envelope.request.params["id"] = str(log.id)
                    envelope.request.params["role"] = log.role
                    envelope.request.params["content"] = log.content
                    envelope.request.params["timestamp"] = log.timestamp.isoformat() if log.timestamp else ""
                    envelope.request.params["agent_id"] = log.agent_id or ""
                    envelope.request.params["session_id"] = log.session_id or ""
                    envelope.request.params["metadata"] = log.metadata_json or "{}"
                    
                    await gateway_client.send(envelope)

                await broadcast_log(user_log)
                await broadcast_log(assistant_log)
            except Exception as gw_e:
                print(f"[MemoryService] Broadcast failed: {gw_e}")

            return user_log, assistant_log
        except Exception as e:
            await session.rollback()
            print(f"[MemoryService] ä¿å­˜æ—¥å¿—å¯¹å¤±è´¥: {e}")
            raise e

    @staticmethod
    async def search_logs(
        session: AsyncSession, 
        query: str, 
        source: Optional[str] = None, 
        limit: int = 10,
        agent_id: Optional[str] = None
    ) -> List[ConversationLog]:
        """
        æŒ‰å…³é”®å­—æœç´¢å¯¹è¯è®°å½•ã€‚
        æ”¯æŒæŒ‰æ¥æºè¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼Œ'qq_%' è¡¨ç¤ºæ‰€æœ‰ qq è®°å½•ï¼‰ã€‚
        """
        statement = select(ConversationLog).order_by(desc(ConversationLog.timestamp))
        
        # [Security] Work Mode Isolation
        # Ensure work mode logs (starting with work_) are NOT queryable via search
        # This prevents them from appearing in Dashboard global search or lists.
        # Work mode logs should only be accessed via get_recent_logs with explicit session_id.
        statement = statement.where(~ConversationLog.session_id.startswith("work_"))
        
        if agent_id:
            statement = statement.where(ConversationLog.agent_id == agent_id)

        if source:
            if "%" in source:
                statement = statement.where(ConversationLog.source.like(source))
            else:
                statement = statement.where(ConversationLog.source == source)
                
        if query:
            statement = statement.where(ConversationLog.content.contains(query))
        
        statement = statement.limit(limit)
        return (await session.exec(statement)).all()

    @staticmethod
    async def get_recent_logs(session: AsyncSession, source: str, session_id: str, limit: int = 20, offset: int = 0, date_str: str = None, sort: str = "asc", agent_id: str = "pero") -> List[ConversationLog]:
        """è·å–æŒ‡å®šæ¥æºå’Œä¼šè¯çš„æœ€è¿‘å¯¹è¯è®°å½•"""
        from sqlmodel import desc, asc
        from datetime import datetime, time
        
        statement = select(ConversationLog).where(ConversationLog.source == source).where(ConversationLog.session_id == session_id).where(ConversationLog.agent_id == agent_id)
        
        if date_str:
            try:
                # å‡è®¾ date_str æ ¼å¼ä¸º YYYY-MM-DD
                target_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                start_dt = datetime.combine(target_date, time.min)
                end_dt = datetime.combine(target_date, time.max)
                statement = statement.where(ConversationLog.timestamp >= start_dt).where(ConversationLog.timestamp <= end_dt)
            except ValueError:
                print(f"[MemoryService] æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {date_str}")

        if sort == "desc":
            statement = statement.order_by(desc(ConversationLog.timestamp), desc(ConversationLog.id))
        else:
            statement = statement.order_by(desc(ConversationLog.timestamp), desc(ConversationLog.id))
            
        statement = statement.offset(offset).limit(limit)
        logs = (await session.exec(statement)).all()
        
        # å¦‚æœæ˜¯æ­£åºæ’åˆ—ï¼Œæˆ‘ä»¬éœ€è¦åè½¬ç»“æœï¼Œå› ä¸º limit æ˜¯å–æœ€æ–°çš„
        if sort == "asc":
            return list(reversed(logs))
        return list(logs)

    @staticmethod
    async def delete_log(session: AsyncSession, log_id: int):
        """åˆ é™¤æŒ‡å®šçš„å¯¹è¯è®°å½• (å¦‚æœå±äºæˆå¯¹è®°å½•ï¼Œåˆ™æˆå¯¹åˆ é™¤)"""
        log = await session.get(ConversationLog, log_id)
        if not log:
            return
            
        if log.pair_id:
            # å¦‚æœæœ‰ pair_idï¼Œåˆ é™¤è¯¥ç»„å†…çš„æ‰€æœ‰è®°å½•
            statement = delete(ConversationLog).where(ConversationLog.pair_id == log.pair_id)
        else:
            # å¦åˆ™ä»…åˆ é™¤å•æ¡
            statement = delete(ConversationLog).where(ConversationLog.id == log_id)
            
        await session.exec(statement)
        await session.commit()

    @staticmethod
    async def update_log(session: AsyncSession, log_id: int, content: str) -> Optional[ConversationLog]:
        """æ›´æ–°æŒ‡å®šçš„å¯¹è¯è®°å½•å†…å®¹"""
        log = await session.get(ConversationLog, log_id)
        if log:
            log.content = content
            session.add(log)
            await session.commit()
            await session.refresh(log)
        return log

    @staticmethod
    async def delete_by_msg_timestamp(session: AsyncSession, msg_timestamp: str):
        statement = delete(Memory).where(Memory.msgTimestamp == msg_timestamp)
        await session.exec(statement)
        await session.commit()

    @staticmethod
    async def mark_memories_accessed(session: AsyncSession, memories: List[Memory]):
        """
        [Reinforcement]
        æ ‡è®°è®°å¿†è¢«è®¿é—®ï¼Œå¢åŠ  access_count å¹¶å°å¹…æå‡ base_importance
        """
        from datetime import datetime
        if not memories:
            return

        for m in memories:
            if m.access_count is None:
                m.access_count = 0
            m.access_count += 1
            m.last_accessed = datetime.now()
            # æ¯æ¬¡è®¿é—®æå‡ 0.1ï¼Œä¸Šé™ 10.0
            if m.base_importance < 10.0:
                m.base_importance = min(10.0, m.base_importance + 0.1)
                m.importance = int(m.base_importance) # åŒæ­¥æ•´æ•° importance
            session.add(m)
        
        try:
            await session.commit()
        except Exception as e:
            print(f"[MemoryService] æ›´æ–°è®¿é—®ç»Ÿè®¡å¤±è´¥: {e}")

    @staticmethod
    async def logical_flashback(session: AsyncSession, text: str, limit: int = 5, agent_id: str = "pero") -> List[Dict[str, Any]]:
        """
        [Brain-Net Flashback] 
        åŸºäºå½“å‰å¯¹è¯å…³é”®è¯ï¼Œåœ¨è®°å¿†å›¾è°±ä¸­è¿›è¡Œè”æƒ³é—ªå›ï¼Œæ‰¾å›å…³è”çš„ç¢ç‰‡ä¿¡æ¯ã€‚
        """
        if not text or len(text.strip()) < 2:
            return []

        from services.embedding_service import embedding_service
        from services.vector_service import vector_service
        import numpy as np

        try:
            # 1. å‘é‡æœç´¢æ‰¾åˆ°åˆå§‹é”šç‚¹ (Anchors)
            query_vec = embedding_service.encode_one(text)
            if not query_vec:
                print("[Memory] é€»è¾‘é—ªå›: æŸ¥è¯¢å‘é‡ä¸ºç©º")
                return []
            
            # å¬å›ç¨å¾®å¤šä¸€ç‚¹ï¼Œä½œä¸ºæ‰©æ•£èµ·ç‚¹
            vector_results = vector_service.search(query_vec, limit=10, agent_id=agent_id)
            if not vector_results:
                print("[Memory] é€»è¾‘é—ªå›: æœªæ‰¾åˆ°å‘é‡ç»“æœ")
                return []

            anchor_ids = [res["id"] for res in vector_results]
            sim_map = {res["id"]: res["score"] for res in vector_results}

            # 2. æ‰©æ•£æ¿€æ´» (Spreading Activation)
            activation_scores = {aid: sim_map.get(aid, 0.5) for aid in anchor_ids}
            
            engine = await get_rust_engine(session)
            if engine:
                # æ‰©æ•£ 2 æ­¥ï¼Œæ‰©å¤§è”æƒ³èŒƒå›´
                print(f"[Memory] æ­£åœ¨ä»é”šç‚¹æ‰©æ•£æ¿€æ´»: {anchor_ids}")
                flashback_scores = engine.propagate_activation(
                    activation_scores,
                    steps=2,
                    decay=0.7,
                    min_threshold=0.05
                )
                print(f"[Memory] æ‰©æ•£ç»“æœæ•°é‡: {len(flashback_scores)}")
            else:
                print("[Memory] Rust å¼•æ“ä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨é”šç‚¹")
                flashback_scores = activation_scores

            # 3. æå– Top å…³è”è®°å¿†å¹¶è½¬æ¢ä¸ºç¢ç‰‡æ ‡ç­¾
            # æ’é™¤æ‰åˆå§‹é”šç‚¹ï¼Œå¯»æ‰¾è¢«â€œè”æƒ³â€å‡ºæ¥çš„ä¸œè¥¿
            associated_ids = [mid for mid in flashback_scores.keys() if mid not in anchor_ids]
            if not associated_ids:
                # å¦‚æœæ²¡æœ‰è”æƒ³å‡ºæ–°ä¸œè¥¿ï¼Œå°±ç”¨åˆå§‹é”šç‚¹ä¸­åˆ†æ•°æœ€é«˜çš„
                associated_ids = anchor_ids

            # æŒ‰åˆ†æ•°æ’åº
            sorted_ids = sorted(associated_ids, key=lambda x: flashback_scores.get(x, 0), reverse=True)[:limit]
            
            if not sorted_ids:
                return []

            # è·å–è®°å¿†è¯¦æƒ…
            statement = select(Memory).where(Memory.id.in_(sorted_ids)).where(Memory.agent_id == agent_id)
            memories = (await session.exec(statement)).all()
            
            # è½¬æ¢ä¸ºç¢ç‰‡æ ¼å¼ (ä¸»è¦æ˜¯æ ‡ç­¾æˆ–çŸ­å¥)
            results = []
            seen_tags = set()
            for m in memories:
                # ä¼˜å…ˆä½¿ç”¨æ ‡ç­¾
                if m.tags:
                    tags = [t.strip() for t in m.tags.split(",") if t.strip()]
                    for tag in tags:
                        if tag not in seen_tags:
                            results.append({"id": m.id, "name": tag, "type": "tag"})
                            seen_tags.add(tag)
                
                # å¦‚æœæ ‡ç­¾ä¸å¤Ÿï¼Œæˆ–è€…ä¸ºäº†ä¸°å¯Œåº¦ï¼ŒåŠ å…¥å†…å®¹æ‘˜è¦
                if len(results) < limit:
                    summary = m.content[:20] + "..." if len(m.content) > 20 else m.content
                    results.append({"id": m.id, "name": summary, "type": "memory"})
            
            return results[:limit]

        except Exception as e:
            print(f"[Memory] é€»è¾‘é—ªå›å¤±è´¥: {e}")
            return []

    @staticmethod
    async def get_relevant_memories(
        session: AsyncSession, 
        text: str, 
        limit: int = 5,
        query_vec: Optional[List[float]] = None,
        exclude_after_time: Optional[datetime] = None,
        update_access_stats: bool = True, # æ–°å¢å‚æ•°ä»¥æ§åˆ¶å‰¯ä½œç”¨
        agent_id: str = "pero"
    ) -> List[Memory]:
        """
        [é“¾ç½‘æ£€ç´¢ V3] (å¯ç”¨ VectorDB + ç°‡è½¯åŠ æƒ)
        1. åµŒå…¥æœç´¢ (VectorDB)
        2. æ‰©æ•£æ¿€æ´» (é“¾)
        3. ç°‡è½¯åŠ æƒé‡æ’åº
        """
        from services.embedding_service import embedding_service
        from services.vector_service import vector_service
        from utils.memory_file_manager import MemoryFileManager
        import numpy as np
        import math
        import os
        import re

        # --- 0. æ„å›¾è¯†åˆ«ä¸ç°‡æ„ŸçŸ¥ (Intent Detection) ---
        # ç®€å•è§„åˆ™åŒ¹é…ï¼šæ ¹æ® Query å…³é”®è¯é¢„æµ‹å½“å‰æ„å›¾ç°‡
        # åœ¨æœªæ¥å¯ä»¥æ›¿æ¢ä¸ºè½»é‡çº§åˆ†ç±»æ¨¡å‹
        target_cluster = None
        cluster_keywords = {
            "é€»è¾‘æ¨ç†ç°‡": ["æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "ä»£ç ", "bug", "é€»è¾‘", "åˆ†æ", "åŸç†", "è§£é‡Š", "define", "function"],
            "æƒ…æ„Ÿåå¥½ç°‡": ["å–œæ¬¢", "è®¨åŒ", "çˆ±", "æ¨", "æ„Ÿè§‰", "å¿ƒæƒ…", "å¼€å¿ƒ", "éš¾è¿‡", "è§‰å¾—", "want", "hate", "love"],
            "è®¡åˆ’æ„å›¾ç°‡": ["æ‰“ç®—", "è®¡åˆ’", "å‡†å¤‡", "æ˜å¤©", "ä¸‹å‘¨", "æœªæ¥", "ç›®æ ‡", "todo", "plan", "will"],
            "åˆ›é€ çµæ„Ÿç°‡": ["æƒ³æ³•", "ç‚¹å­", "æ•…äº‹", "å¦‚æœ", "å‡è®¾", "è„‘æ´", "idea", "imagine", "story"],
            "åæ€ç°‡": ["é”™äº†", "æ”¹è¿›", "åçœ", "ä¸å¥½", "çƒ‚", "ä¿®æ­£", "sorry", "mistake", "fix"]
        }
        
        if text:
            for cluster, keywords in cluster_keywords.items():
                if any(k in text.lower() for k in keywords):
                    target_cluster = cluster
                    break
        
        if target_cluster:
            # print(f"[Memory] Detected Intent Cluster: {target_cluster}")
            pass

        # 1. å‘é‡åŒ– Query (å¦‚æœæ²¡æœ‰ä¼ å…¥é¢„è®¡ç®—çš„å‘é‡)
        if query_vec is None:
            if not text:
                return []
            query_vec = embedding_service.encode_one(text)
            
        if not query_vec:
            print("[Memory] Embedding å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢ã€‚")
            if text:
                return await MemoryService._keyword_search_fallback(session, text, limit, exclude_after_time, agent_id=agent_id)
            return []

        # 2. å‘é‡æ£€ç´¢ (VectorDB Search)
        try:
            # [Optimization] æ‰©å¤§å¬å›èŒƒå›´è‡³ 60ï¼Œä»¥ä¾¿åœ¨è¿‡æ»¤æ‰è¿‘æœŸè®°å¿†ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰åä»æœ‰è¶³å¤Ÿçš„å€™é€‰
            vector_results = vector_service.search(query_vec, limit=60, agent_id=agent_id) 
            
            if not vector_results:
                # å°è¯•ä» SQLite å›é€€ (å¦‚æœæ˜¯è¿ç§»è¿‡æ¸¡æœŸ)
                print("[Memory] VectorDB æœªè¿”å›ç»“æœï¼Œå°è¯• SQLite å›é€€...")
                fallback_res = await MemoryService._keyword_search_fallback(session, text, limit, exclude_after_time, agent_id=agent_id)
                if update_access_stats and fallback_res:
                    await MemoryService.mark_memories_accessed(session, fallback_res)
                return fallback_res

            # è·å– Memory å¯¹è±¡
            # æå– ID åˆ—è¡¨
            memory_ids = [res["id"] for res in vector_results]
            
            if not memory_ids:
                 return []
                 
            statement = select(Memory).where(Memory.id.in_(memory_ids)).where(Memory.agent_id == agent_id)
            valid_memories = (await session.exec(statement)).all()
            
            # å»ºç«‹ ID -> Similarity æ˜ å°„
            sim_map = {res["id"]: res["score"] for res in vector_results}
            
            # [Context Awareness] è¿‡æ»¤æ‰ exclude_after_time (å³ä¸Šä¸‹æ–‡çª—å£å†…çš„è®°å¿†)
            if exclude_after_time:
                exclude_ts = exclude_after_time.timestamp() * 1000
                original_count = len(valid_memories)
                valid_memories = [m for m in valid_memories if m.timestamp < exclude_ts]
                filtered_count = len(valid_memories)
                if original_count != filtered_count:
                    print(f"[Memory] ä¸Šä¸‹æ–‡è¿‡æ»¤: æ’é™¤äº† {original_count - filtered_count} æ¡ä¸ä¸Šä¸‹æ–‡çª—å£é‡å çš„è®°å¿†ã€‚")

            # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºï¼ˆç¬¦åˆç”¨æˆ·æœŸæœ›ï¼šè‹¥é•¿è®°å¿†æ¡ç›®å…¨åœ¨ä¸Šä¸‹æ–‡çª—å£å†…ï¼Œåˆ™è·³è¿‡æ£€ç´¢ï¼‰
            if not valid_memories:
                return []

        except Exception as e:
            print(f"[Memory] VectorDB æœç´¢å¤±è´¥: {e}ã€‚æ­£åœ¨å›é€€ã€‚")
            fallback_res = await MemoryService._keyword_search_fallback(session, text, limit, exclude_after_time, agent_id=agent_id)
            if update_access_stats and fallback_res:
                await MemoryService.mark_memories_accessed(session, fallback_res)
            return fallback_res
        
        # 3. æ‰©æ•£æ¿€æ´» (Spreading Activation)
        # åˆå§‹æ¿€æ´»å€¼ = VectorDB Similarity
        activation_scores = {m.id: sim_map.get(m.id, 0.0) for m in valid_memories}
        
        # è·å–æ‰€æœ‰å…³è” (ä¸€æ¬¡æ€§æ‹‰å–ï¼Œé¿å… N+1)
        # ç®€å•èµ·è§ï¼Œè¿™é‡Œåªå¯¹ Top N çš„ Anchor è¿›è¡Œæ‰©æ•£
        # é€‰å‡º Top 20 Anchors (è¿™é‡Œ valid_memories å·²ç»æ˜¯ Top N äº†)
        anchors = valid_memories
        anchor_ids = [m.id for m in anchors]
        
        # [Optimized] æ‰¹é‡æ‹‰å–æ‰€æœ‰ç›¸å…³å…³ç³»
        rust_relations = []
        
        if anchor_ids:
            # æŸ¥æ‰¾æ‰€æœ‰ source æˆ– target åœ¨ anchor_ids ä¸­çš„å…³ç³»
            statement = select(MemoryRelation).where(
                (MemoryRelation.source_id.in_(anchor_ids)) | 
                (MemoryRelation.target_id.in_(anchor_ids))
            )
            all_relations = (await session.exec(statement)).all()
            
            # æ„å»ºå…³ç³»åˆ—è¡¨ (source, target, weight)
            for rel in all_relations:
                rust_relations.append((rel.source_id, rel.target_id, rel.strength * 0.5))
        
        # æ·»åŠ  Prev/Next å…³ç³»
        for anchor in anchors:
            if anchor.prev_id:
                rust_relations.append((anchor.id, anchor.prev_id, 0.2))
            if anchor.next_id:
                rust_relations.append((anchor.id, anchor.next_id, 0.2))

        # [Rust é›†æˆ] é’ˆå¯¹ç™¾ä¸‡çº§èŠ‚ç‚¹ä¼˜åŒ–
        try:
            engine = await get_rust_engine(session, agent_id=agent_id)
            if engine:
                # æ‰§è¡Œæ‰©æ•£ï¼šå¼•å…¥åŠ¨æ€é˜ˆå€¼ min_threshold
                # å¦‚æœæ˜¯é‡è¦æŸ¥è¯¢ï¼Œå¯ä»¥è°ƒä½é˜ˆå€¼ä»¥è·å–æ›´å¤šè”æƒ³ï¼›å¦åˆ™ä¿æŒ 0.01 ä¿è¯æ€§èƒ½
                new_scores = engine.propagate_activation(
                    activation_scores, 
                    steps=1, 
                    decay=1.0, 
                    min_threshold=0.01
                )
                activation_scores = new_scores
            else:
                # å¼•æ“ä¸å¯ç”¨æ—¶çš„å›é€€é€»è¾‘
                pass
        except Exception as e:
            print(f"[Memory] Rust engine runtime error: {e}. Falling back.")
            # [å›é€€åˆ° Python]
            # print("[Memory] Rust engine not found. Using Python fallback.")
            relation_map = {}
            for rel in all_relations: # é‡ç”¨æ•°æ®åº“ç»“æœ
                if rel.source_id in activation_scores:
                    relation_map.setdefault(rel.source_id, []).append(rel)
                if rel.target_id in activation_scores:
                    relation_map.setdefault(rel.target_id, []).append(rel)

            for anchor in anchors:
                base_score = activation_scores[anchor.id]
                if base_score < 0.3: continue

                # A. æ—¶é—´è½´æ‰©æ•£ (Prev/Next)
                if anchor.prev_id and anchor.prev_id in activation_scores:
                    activation_scores[anchor.prev_id] += base_score * 0.2
                if anchor.next_id and anchor.next_id in activation_scores:
                    activation_scores[anchor.next_id] += base_score * 0.2

                # B. å…³ç³»ç½‘æ‰©æ•£
                relations = relation_map.get(anchor.id, [])
                for rel in relations:
                    target_id = rel.target_id if rel.source_id == anchor.id else rel.source_id
                    if target_id in activation_scores:
                        activation_scores[target_id] += base_score * rel.strength * 0.5
        except Exception as e:
            # æè‡´ç¨³å®šæ€§ï¼šå¦‚æœ Rust å¼•æ“è¿è¡ŒæŠ¥é”™ï¼ˆå¦‚ OOMï¼‰ï¼Œè®°å½•æ—¥å¿—å¹¶ç»§ç»­ï¼Œä¸ä¸­æ–­å¯¹è¯
            print(f"[Memory] Rust engine runtime error: {e}. Falling back to initial scores.")
            # æ­¤æ—¶ä¿æŒ activation_scores ä¸å˜ï¼ˆå³ä»…ä½¿ç”¨å‘é‡æœç´¢ç»“æœï¼‰

        # 4. ç»¼åˆæ’åº (æœ€ç»ˆæ’å) å¸¦æ—¶é—´è¡°å‡å’Œç°‡è½¯åŠ æƒ
        # Score = (Sim * w1) + (ClusterBonus) + (Importance * w2) * Decay(t) + (Recency * w3)
        final_candidates = []
        current_time = datetime.now().timestamp() * 1000
        
        for m in valid_memories:
            # åŸºç¡€ç›¸å…³åº¦åˆ†æ•° (Sim)
            act_score = activation_scores.get(m.id, 0.0)
            
            # [Feature] Cluster Soft-Weighting (ç°‡æ„ŸçŸ¥è½¯åŠ æƒ)
            # å¦‚æœè®°å¿†çš„ç°‡ä¸å½“å‰æ„å›¾ç°‡åŒ¹é…ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
            cluster_bonus = 0.0
            if target_cluster and m.clusters and target_cluster in m.clusters:
                cluster_bonus = 0.15 # ç°‡åŒ¹é…å¢åŠ  15% å¥–åŠ±
                # print(f"[Memory] Cluster Bonus Applied: +0.15 for {m.id} (Match: {target_cluster})")
            
            # å½’ä¸€åŒ–é‡è¦æ€§ (Importance)
            imp_score = min(m.base_importance, 10.0) / 10.0
            
            # è‰¾å®¾æµ©æ–¯è¡°å‡: exp(-lambda * delta_t)
            time_diff_ms = max(0, current_time - m.timestamp)
            time_diff_days = time_diff_ms / (1000 * 3600 * 24)
            decay_factor = math.exp(-0.023 * time_diff_days) # 30å¤©çº¦è¡°å‡è‡³ 0.5
            
            # Recency Bonus (è¿‘æœŸæ€§å¥–åŠ±): è¶Šæ–°çš„è®°å¿†å¥–åŠ±è¶Šé«˜
            # è®¾å®š 1 å°æ—¶å†…çš„è®°å¿†å¥–åŠ± 0.2ï¼Œéšæ—¶é—´çº¿æ€§è¡°å‡è‡³ 0
            recency_bonus = max(0, 0.2 * (1 - time_diff_days / 1.0)) if time_diff_days < 1.0 else 0
            
            # ä¸¥æ ¼éµå¾ªè®¾è®¡å…¬å¼:
            # Score = (ç›¸å…³åº¦ * 0.7) + ClusterBonus + (é‡è¦æ€§ * 0.3 * è¡°å‡) + è¿‘æœŸå¥–åŠ±
            final_score = (act_score * 0.7) + cluster_bonus + (imp_score * 0.3 * decay_factor) + recency_bonus
            
            if final_score > 0.1: # ç•¥å¾®é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šå€™é€‰è¿›å…¥ Rerank
                final_candidates.append((m, final_score))

        # 5. Rerank
        # æŒ‰ç»¼åˆå¾—åˆ†åˆæ­¥ç­›é€‰
        final_candidates.sort(key=lambda x: x[1], reverse=True)
        top_candidates = [item[0] for item in final_candidates[:limit*2]]
        
        result_memories = []
        if top_candidates:
            try:
                docs = [m.content for m in top_candidates]
                rerank_results = embedding_service.rerank(text, docs, top_k=limit)
                
                # æ ¹æ® Rerank ç»“æœé‡æ–°ç»„è£…
                for res in rerank_results:
                    original_idx = res["index"]
                    result_memories.append(top_candidates[original_idx])
            except Exception as e:
                print(f"[Memory] é‡æ’åºå¤±è´¥: {e}ã€‚å›é€€åˆ°åˆå§‹åˆ†æ•°ã€‚")
                result_memories = top_candidates[:limit]
            final_memories = result_memories
        else:
            result_memories = top_candidates[:limit]
            final_memories = result_memories

        # [Hydrate] å¦‚æœæ˜¯å½’æ¡£è®°å¿†ï¼Œå®æ—¶è¯»å–æ–‡ä»¶å†…å®¹
        # æ¨¡å¼åŒ¹é… "> ğŸ“ File Archived: path"
        # æ³¨æ„ï¼šç›®å‰æˆ‘ä»¬åŒæ­¥æ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºå¯¹äºå°‘é‡è®°å¿†ï¼Œæ–‡ä»¶ IO è¶³å¤Ÿå¿«ï¼Œ
        # æˆ–è€…å¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ aiofilesã€‚ä½† MemoryFileManager åœ¨ _write_file (å°è£…åœ¨ to_thread ä¸­) ä¸­ä½¿ç”¨é˜»å¡æ‰“å¼€ã€‚
        # åœ¨è¿™é‡Œè¯»å–æ›´å®‰å…¨ã€‚
        for m in result_memories:
            if "> ğŸ“ File Archived:" in m.content:
                try:
                    match = re.search(r"> ğŸ“ File Archived: (.+)", m.content)
                    if match:
                        file_path = match.group(1).strip()
                        if os.path.exists(file_path):
                            with open(file_path, 'r', encoding='utf-8') as f:
                                file_content = f.read()
                                # ä¿ç•™ä¸€ç‚¹ DB ä¸­çš„å…ƒæ•°æ®æç¤ºï¼Œä½†ç”¨æ–‡ä»¶å†…å®¹è¦†ç›–ä¸»ä½“
                                # æˆ–è€…ç›´æ¥æ›¿æ¢
                                m.content = file_content 
                except Exception as e:
                    print(f"[MemoryService] å¡«å……å½’æ¡£è®°å¿† {m.id} å¤±è´¥: {e}")

        # [ä¿®å¤] æ›´æ–°è®¿é—®ç»Ÿè®¡ (å¼ºåŒ–)
        # åªè¦è¢«æ£€ç´¢åˆ°å¹¶æœ€ç»ˆè¿”å›ï¼Œå°±è§†ä¸ºè¢«"æ¿€æ´»"äº†ä¸€æ¬¡
        if update_access_stats and result_memories:
            # åŒæ­¥ç­‰å¾…æ›´æ–°å®Œæˆï¼Œé˜²æ­¢ session æå‰å…³é—­
            await MemoryService.mark_memories_accessed(session, result_memories)

        return result_memories

    @staticmethod
    async def get_memories_by_filter(
        session: AsyncSession, 
        limit: int = 10, 
        filter_criteria: Dict = None,
        agent_id: str = "pero"
    ) -> List[Dict]:
        """
        åŸºäº Metadata è¿‡æ»¤è®°å¿† (ç”¨äºå‘¨æŠ¥ç”Ÿæˆç­‰)
        æ›¿ä»£ vector_service.query_memories
        """
        statement = select(Memory).where(Memory.agent_id == agent_id)
        
        if filter_criteria:
            # æ—¶é—´æˆ³èŒƒå›´çš„ç®€å•å®ç°
            # {"timestamp": {"$lt": ...}}
            ts_filter = filter_criteria.get("timestamp")
            if ts_filter and isinstance(ts_filter, dict):
                lt_val = ts_filter.get("$lt")
                gt_val = ts_filter.get("$gt")
                if lt_val:
                    statement = statement.where(Memory.timestamp < lt_val)
                if gt_val:
                    statement = statement.where(Memory.timestamp > gt_val)
                    
            # TODO: å¦‚æœéœ€è¦ï¼Œå¤„ç†å…¶ä»–è¿‡æ»¤å™¨ï¼Œå¦‚æ ‡ç­¾/ç°‡
        
        statement = statement.order_by(desc(Memory.timestamp)).limit(limit)
        results = await session.exec(statement)
        memories = results.all()
        
        # è½¬æ¢ä¸º ChainService æœŸæœ›çš„å­—å…¸æ ¼å¼
        output = []
        for m in memories:
            output.append({
                "id": m.id,
                "document": m.content,
                "metadata": {
                    "timestamp": m.timestamp,
                    "importance": m.importance,
                    "tags": m.tags,
                    "type": m.type
                }
            })
        return output

    @staticmethod
    async def search_memories_simple(
        session: AsyncSession,
        query_vec: List[float],
        limit: int = 5,
        filter_criteria: Dict = None,
        agent_id: str = "pero"
    ) -> List[Dict]:
        """
        ç®€å•çš„å‘é‡æœç´¢ + Metadata è¿‡æ»¤ (ç”¨äº ChainService æŸ¥æ‰¾å†å²)
        """
        from services.vector_service import vector_service
        
        # 1. æœç´¢ VectorDB (è·å–æ›´å¤šå€™é€‰ä»¥å…è®¸è¿‡æ»¤)
        # HACK: Rust ç´¢å¼•ä¸æ”¯æŒé¢„è¿‡æ»¤ï¼Œæ‰€ä»¥æˆ‘ä»¬è·å–æ›´å¤šå¹¶è¿›è¡Œåè¿‡æ»¤ã€‚
        candidates = vector_service.search(query_vec, limit=limit * 5, agent_id=agent_id)
        if not candidates: return []
        
        ids = [c["id"] for c in candidates]
        score_map = {c["id"]: c["score"] for c in candidates}
        
        # 2. ä»å¸¦è¿‡æ»¤å™¨çš„ DB ä¸­è·å–
        statement = select(Memory).where(Memory.id.in_(ids)).where(Memory.agent_id == agent_id)
        
        if filter_criteria:
            ts_filter = filter_criteria.get("timestamp")
            if ts_filter and isinstance(ts_filter, dict):
                lt_val = ts_filter.get("$lt")
                if lt_val:
                    statement = statement.where(Memory.timestamp < lt_val)
        
        results = await session.exec(statement)
        memories = results.all()
        
        # 3. æ ¼å¼åŒ–
        output = []
        for m in memories:
            output.append({
                "id": m.id,
                "score": score_map.get(m.id, 0),
                "document": m.content,
                "metadata": {
                    "timestamp": m.timestamp,
                    "importance": m.importance
                }
            })
            
        # æŒ‰åˆ†æ•°æ’åº
        output.sort(key=lambda x: x["score"], reverse=True)
        return output[:limit]

            
        return top_candidates[:limit]

    @staticmethod
    async def _keyword_search_fallback(session: AsyncSession, text: str, limit: int = 10, exclude_after_time=None, agent_id: str = "pero") -> List[Memory]:
        """åŸæœ‰çš„å…³é”®è¯æœç´¢é€»è¾‘ï¼Œä½œä¸ºå…œåº•"""
        # ... (ä¿ç•™åŸæœ‰é€»è¾‘)
        # æå–å…³é”®è¯ (ç®€å•æ­£åˆ™åˆ†è¯)
        keywords = [k.lower() for k in re.split(r'[\s,ï¼Œ.ã€‚!ï¼?ï¼Ÿ;ï¼›:ï¼šã€]+', text) if len(k) >= 2]
        
        if not keywords:
            statement = select(Memory).where(Memory.agent_id == agent_id).order_by(Memory.importance.desc()).limit(limit)
            memories = (await session.exec(statement)).all()
        else:
            statement = select(Memory).where(Memory.agent_id == agent_id)
            all_memories = (await session.exec(statement)).all()

            scored_memories = []
            for m in all_memories:
                score = 0
                m_tags = [t.lower() for t in (m.tags.split(',') if m.tags else [])]
                
                for kw in keywords:
                    if any(kw in t or t in kw for t in m_tags):
                        score += 10
                    if kw in m.content.lower():
                        score += 5
                
                score += m.importance
                if score > 0:
                    scored_memories.append((m, score))

            scored_memories.sort(key=lambda x: x[1], reverse=True)
            memories = [m for m, s in scored_memories[:limit]]

        if exclude_after_time and memories:
            exclude_timestamp_ms = exclude_after_time.timestamp() * 1000
            memories = [m for m in memories if m.timestamp < exclude_timestamp_ms]

        return memories

    @staticmethod
    async def get_all_memories(
        session: AsyncSession, 
        limit: int = 50, 
        offset: int = 0, 
        date_start: str = None, 
        date_end: str = None, 
        tags: str = None,
        memory_type: str = None,
        agent_id: str = None # Allow filtering by agent
    ) -> List[Memory]:
        from datetime import datetime
        import time
        
        statement = select(Memory)
        
        # Agent Filter
        if agent_id:
            statement = statement.where(Memory.agent_id == agent_id)
        
        # ç±»å‹è¿‡æ»¤å™¨
        if memory_type:
            statement = statement.where(Memory.type == memory_type)
        
        # æ—¥æœŸè¿‡æ»¤å™¨ (ä½¿ç”¨æ—¶é—´æˆ³ ms)
        if date_start:
            try:
                start_dt = datetime.strptime(date_start, "%Y-%m-%d")
                start_ms = start_dt.timestamp() * 1000
                statement = statement.where(Memory.timestamp >= start_ms)
            except Exception as e:
                print(f"[MemoryService] æ— æ•ˆçš„å¼€å§‹æ—¥æœŸ: {e}")
                
        if date_end:
            try:
                end_dt = datetime.strptime(date_end, "%Y-%m-%d")
                # å¢åŠ ä¸€å¤©ä»¥å®Œå…¨åŒ…å«ç»“æŸæ—¥æœŸ
                end_ms = (end_dt.timestamp() + 86400) * 1000
                statement = statement.where(Memory.timestamp < end_ms)
            except Exception as e:
                print(f"[MemoryService] æ— æ•ˆçš„ç»“æŸæ—¥æœŸ: {e}")
        
        # æ ‡ç­¾è¿‡æ»¤å™¨ (ç®€å•çš„å­—ç¬¦ä¸²åŒ…å«)
        if tags:
            tag_list = [t.strip() for t in tags.split(',') if t.strip()]
            for tag in tag_list:
                statement = statement.where(Memory.tags.contains(tag))

        statement = statement.order_by(desc(Memory.timestamp)).offset(offset).limit(limit)
        return (await session.exec(statement)).all()

    @staticmethod
    async def get_tag_cloud(session: AsyncSession, agent_id: str = "pero") -> List[Dict[str, Any]]:
        """
        è·å–æ ‡ç­¾äº‘æ•°æ® (Top 20 tags)
        """
        # ç®€å•å®ç°ï¼šå–å‡ºæ‰€æœ‰ Memory çš„ tags å­—æ®µï¼Œåœ¨å†…å­˜ä¸­ç»Ÿè®¡
        # TODO: åæœŸå¯ä»¥ä½¿ç”¨ SQL group by ä¼˜åŒ–
        statement = select(Memory)
        if agent_id:
            statement = statement.where(Memory.agent_id == agent_id)
            
        memories = (await session.exec(statement)).all()
        tag_counts = {}
        
        for m in memories:
            if not m.tags: continue
            tags = [t.strip() for t in m.tags.split(',') if t.strip()]
            for t in tags:
                tag_counts[t] = tag_counts.get(t, 0) + 1
                
        # æ’åºå¹¶å–å‰ 20
        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        return [{"tag": t, "count": c} for t, c in sorted_tags]

    @staticmethod
    async def delete_orphaned_edges(session: AsyncSession) -> int:
        """
        æ¸…é™¤å­¤ç«‹çš„è¾¹ï¼ˆå³æºèŠ‚ç‚¹æˆ–ç›®æ ‡èŠ‚ç‚¹ä¸å­˜åœ¨çš„è¾¹ï¼‰
        """
        # ä½¿ç”¨å­æŸ¥è¯¢æŸ¥æ‰¾ä¸å­˜åœ¨çš„èŠ‚ç‚¹å¼•ç”¨
        # DELETE FROM memoryrelation WHERE source_id NOT IN (SELECT id FROM memory) OR target_id NOT IN (SELECT id FROM memory)
        
        # SQLModel çš„ delete æ”¯æŒ where å­å¥ï¼Œä½†å¯¹ subquery æ”¯æŒè§†æ–¹è¨€è€Œå®š
        # è¿™é‡Œä½¿ç”¨æ ‡å‡† SQLAlchemy é£æ ¼
        
        subquery = select(Memory.id)
        
        statement = delete(MemoryRelation).where(
            (MemoryRelation.source_id.not_in(subquery)) | 
            (MemoryRelation.target_id.not_in(subquery))
        )
        
        result = await session.exec(statement)
        await session.commit()
        
        # å¦‚æœæœ‰ Rust Engine ä¸”å·²åŠ è½½ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½æˆ–åŒæ­¥åˆ é™¤
        # ç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾ Rust Engine ä¼šåœ¨ä¸‹æ¬¡å¯åŠ¨æˆ–å®šæœŸåˆ·æ–°æ—¶åŒæ­¥
        # æˆ–è€…æˆ‘ä»¬å¯ä»¥å°è¯•ä» Rust Engine ä¸­ç§»é™¤ï¼ˆå¦‚æœæ”¯æŒï¼‰
        # ç›®å‰ Rust Engine æ˜¯åªè¯»/è¿½åŠ ä¸ºä¸»ï¼Œæš‚æ—¶å¿½ç•¥å®æ—¶åŒæ­¥
        
        return result.rowcount

    @staticmethod
    async def get_memory_graph(session: AsyncSession, limit: int = 200, agent_id: str = "pero") -> Dict[str, Any]:
        """è¿”å›ç”¨äºå›¾å½¢å¯è§†åŒ–çš„èŠ‚ç‚¹å’Œè¾¹ (é’ˆå¯¹é…·ç‚« UI å¢å¼º)"""
        # è·å–æœ€è¿‘ N æ¡è®°å¿†
        statement = select(Memory).order_by(desc(Memory.timestamp)).limit(limit)
        if agent_id:
            statement = statement.where(Memory.agent_id == agent_id)
            
        memories = (await session.exec(statement)).all()
        if not memories:
            return {"nodes": [], "edges": []}
            
        memory_ids = [m.id for m in memories]
        
        # è·å–è¿æ¥è¿™äº›è®°å¿†çš„å…³ç³»
        rel_statement = select(MemoryRelation).where(
            (MemoryRelation.source_id.in_(memory_ids)) | (MemoryRelation.target_id.in_(memory_ids))
        )
        # å…³ç³»è¡¨ä¹Ÿæœ‰ agent_idï¼Œå¢åŠ è¿‡æ»¤æ›´ä¸¥è°¨ï¼Œè™½ç„¶åŸºäº memory_ids è¿‡æ»¤å·²ç»éšå«äº†éš”ç¦»
        if agent_id:
            rel_statement = rel_statement.where(MemoryRelation.agent_id == agent_id)
            
        relations = (await session.exec(rel_statement)).all()
        
        # æ ¼å¼åŒ–ä¸ºå‰ç«¯æ ¼å¼ (ECharts åŠ›å¯¼å‘å›¾)
        nodes = []
        for m in memories:
            # æ ¹æ®é‡è¦æ€§å’Œè®¿é—®è®¡æ•°è®¡ç®—ç¬¦å·å¤§å°
            # åŸºç¡€å¤§å° 10ï¼Œæœ€å¤§é‡è¦æ€§ 10 -> +20ï¼Œæœ€å¤§è®¿é—®å¯¹æ•°åˆ»åº¦ -> +10
            import math
            size = 10 + (m.importance * 2) + (math.log(m.access_count + 1) * 5)
            size = min(size, 60) # é™åˆ¶å¤§å°

            nodes.append({
                "id": m.id,
                "name": str(m.id), # ECharts çš„å”¯ä¸€åç§°
                "label": {
                    "show": size > 15, # ä»…æ˜¾ç¤ºé‡è¦èŠ‚ç‚¹çš„æ ‡ç­¾
                    "formatter": m.content[:10] + "..." if len(m.content) > 10 else m.content
                },
                "full_content": m.content,
                "category": m.type, # äº‹ä»¶ã€äº‹å®ç­‰
                "value": m.importance,
                "symbolSize": size,
                "sentiment": m.sentiment,
                "tags": m.tags,
                "realTime": m.realTime,
                "access_count": m.access_count,
                # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ æ¯ä¸ªèŠ‚ç‚¹çš„ ECharts ç‰¹å®šæ ·å¼ï¼Œ
                # ä½†æœ€å¥½åœ¨å‰ç«¯ä½¿ç”¨ categories/visualMap å¤„ç†
            })
            
        edges = []
        added_edges = set()

        for r in relations:
            if r.source_id in memory_ids and r.target_id in memory_ids:
                edge_key = f"{r.source_id}-{r.target_id}"
                if edge_key not in added_edges:
                    edges.append({
                        "source": str(r.source_id),
                        "target": str(r.target_id),
                        "value": r.strength,
                        "relation_type": r.relation_type,
                        "lineStyle": {
                            "width": 1 + (r.strength * 4), # 1px åˆ° 5px
                            "curveness": 0.2
                        },
                        "tooltip": {
                            "formatter": f"{r.relation_type}: {r.description or 'No desc'}"
                        }
                    })
                    added_edges.add(edge_key)
                
        # æ—¶é—´é¡ºåºè¾¹ (Next/Prev) - ä½¿å®ƒä»¬å˜å¾—å¾®å¦™
        for m in memories:
            if m.prev_id and m.prev_id in memory_ids:
                 edge_key = f"{m.prev_id}-{m.id}"
                 if edge_key not in added_edges:
                    edges.append({
                        "source": str(m.prev_id),
                        "target": str(m.id),
                        "value": 1,
                        "relation_type": "temporal",
                        "lineStyle": {
                            "width": 1,
                            "color": "#cccccc",
                            "opacity": 0.3,
                            "type": "dashed",
                            "curveness": 0.1
                        }
                    })
                    added_edges.add(edge_key)
        
        return {"nodes": nodes, "edges": edges}

