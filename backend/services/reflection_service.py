import asyncio
from typing import List, Optional
from sqlmodel import select, desc
from sqlmodel.ext.asyncio.session import AsyncSession
from models import Memory, MemoryRelation, Config, AIModelConfig, MaintenanceRecord, ConversationLog
from services.llm_service import LLMService
from services.mdp.manager import mdp
import json
import random
from datetime import datetime, timedelta
from collections import defaultdict

class ReflectionService:
    def __init__(self, session: AsyncSession):
        self.session = session

    async def _get_reflection_config(self):
        """è·å–åæ€æ¨¡å‹é…ç½® (é€šå¸¸æ˜¯æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚ GPT-4/Claude-3.5)"""
        # å¤ç”¨ AgentService ä¸­çš„é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥æŸ¥è¯¢ Config
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥æŸ¥ Config
        configs = {c.key: c.value for c in (await self.session.exec(select(Config))).all()}
        
        reflection_model_id = configs.get("reflection_model_id")
        
        # é»˜è®¤å›é€€åˆ°ä¸»æ¨¡å‹
        main_model_id = configs.get("current_model_id")
        
        api_key = configs.get("global_llm_api_key", "")
        api_base = configs.get("global_llm_api_base", "https://api.openai.com")
        model = "gpt-4o" # Fallback if everything fails, but we try to use main model first

        if reflection_model_id:
            model_config = await self.session.get(AIModelConfig, int(reflection_model_id))
            if model_config:
                api_key = model_config.api_key if model_config.provider_type == 'custom' else api_key
                api_base = model_config.api_base if model_config.provider_type == 'custom' else api_base
                model = model_config.model_id
        elif main_model_id:
             # Use Main Model as fallback
            model_config = await self.session.get(AIModelConfig, int(main_model_id))
            if model_config:
                api_key = model_config.api_key if model_config.provider_type == 'custom' else api_key
                api_base = model_config.api_base if model_config.provider_type == 'custom' else api_base
                model = model_config.model_id
        
        return {
            "api_key": api_key,
            "api_base": api_base,
            "model": model,
            "temperature": 0.4 # éœ€è¦ä¸€å®šçš„åˆ›é€ åŠ›æ¥å‘ç°å…³è”ï¼Œä½†ä¸èƒ½å¤ªå‘æ•£
        }

    async def backfill_failed_scorer_tasks(self, retry_limit: int = 3, concurrency_limit: int = 5):
        """
        [è¡¥å½•è®°å¿†]
        å¤„ç†å¤±è´¥çš„Scoreråˆ†æä»»åŠ¡
        """
        print("[Reflection] Starting failed Scorer tasks backfill...", flush=True)
        
        semaphore = asyncio.Semaphore(concurrency_limit)
        
        async def retry_task(task):
            async with semaphore:
                print(f"[Reflection] Backfilling failed task (ID: {task.id})...")
                # Local import to avoid circular dependency
                from services.scorer_service import ScorerService
                scorer_service = ScorerService(self.session)
                await scorer_service.retry_interaction(task.id)
        
        # 1. æŸ¥æ‰¾å¤±è´¥çš„åˆ†æä»»åŠ¡
        statement = select(ConversationLog).where(
            (ConversationLog.analysis_status == "failed") &
            (ConversationLog.retry_count < retry_limit)
        ).order_by(desc(ConversationLog.timestamp))
        
        failed_tasks = (await self.session.exec(statement)).all()
        if not failed_tasks:
            print("[Reflection] No failed Scorer tasks to backfill.")
            return
        
        # 2. å¹¶å‘å¤„ç†å¤±è´¥ä»»åŠ¡
        tasks = [retry_task(task) for task in failed_tasks]
        await asyncio.gather(*tasks)
        
        await self.session.commit()
        print(f"[Reflection] Backfilled {len(failed_tasks)} failed Scorer tasks.")

    async def consolidate_memories(self, lookback_days: int = 3, importance_threshold: int = 4):
        """
        [Memory Consolidation]
        å‹ç¼©ä½é‡è¦æ€§ã€é™ˆæ—§çš„è®°å¿†ä¸ºé™ˆè¿°æ€§æ€»ç»“ã€‚
        """
        print(f"[Reflection] Starting memory consolidation (older than {lookback_days} days, importance < {importance_threshold})...", flush=True)
        
        # 1. æŸ¥æ‰¾å€™é€‰è®°å¿†
        cutoff_time = datetime.now() - timedelta(days=lookback_days)
        cutoff_timestamp = cutoff_time.timestamp() * 1000
        
        statement = select(Memory).where(
            (Memory.type == "event") & 
            (Memory.timestamp < cutoff_timestamp) & 
            (Memory.importance < importance_threshold)
        ).order_by(Memory.timestamp)
        
        candidates = (await self.session.exec(statement)).all()
        
        if not candidates:
            print("[Reflection] No memories to consolidate.")
            return

        # 2. æŒ‰æ—¥æœŸåˆ†ç»„
        grouped = defaultdict(list)
        for m in candidates:
            # key by YYYY-MM-DD
            date_key = m.realTime.split(" ")[0] if m.realTime else "unknown"
            grouped[date_key].append(m)
            
        config = await self._get_reflection_config()
        if not config["api_key"]:
            print("[Reflection] No API Key, skipping consolidation.")
            return

        llm = LLMService(
            api_key=config["api_key"],
            api_base=config["api_base"],
            model=config["model"]
        )

        # 3. å¤„ç†æ¯ä¸€ç»„
        for date_key, group in grouped.items():
            if len(group) < 3:
                continue # è·³è¿‡å¤ªå°‘çš„ç»„
            
            print(f"[Reflection] Consolidating {len(group)} memories from {date_key}...")
            
            # ç”Ÿæˆæ€»ç»“
            summary_text = await self._generate_summary(llm, group, date_key)
            if not summary_text:
                continue
                
            # Save to File (MD)
            from utils.memory_file_manager import MemoryFileManager
            file_path = await MemoryFileManager.save_log("periodic_summaries", f"{date_key}_Consolidated", summary_text)
            
            # åˆ›å»ºæ€»ç»“æ€§è®°å¿†
            # æˆ‘ä»¬å°†å…¶æ’å…¥åˆ°è¯¥ç»„ç¬¬ä¸€æ¡è®°å¿†çš„ä½ç½®
            first_mem = group[0]
            last_mem = group[-1]
            
            # è®¡ç®—å¹³å‡é‡è¦æ€§å¹¶ç•¥å¾®æå‡
            avg_imp = sum(m.importance for m in group) / len(group)
            new_importance = min(10, int(avg_imp) + 1)
            
            # ç”Ÿæˆ Embedding
            from services.embedding_service import embedding_service
            embedding_json = "[]"
            try:
                vec = embedding_service.encode_one(summary_text)
                embedding_json = json.dumps(vec)
            except: pass
            
            db_content = f"{summary_text}\n\n> ğŸ“ File Archived: {file_path}"

            summary_mem = Memory(
                content=db_content,
                tags="summary,consolidated",
                importance=new_importance,
                base_importance=float(new_importance),
                sentiment="neutral", 
                timestamp=first_mem.timestamp, # ä½¿ç”¨ç¬¬ä¸€æ¡çš„æ—¶é—´
                realTime=first_mem.realTime,
                source="system",
                type="summary",
                embedding_json=embedding_json
            )
            self.session.add(summary_mem)
            await self.session.flush() # è·å– ID
            await self.session.refresh(summary_mem)
            
            # æ›´æ–°é“¾è¡¨ (Bypass the group)
            # A -> [B -> ... -> D] -> E
            # å˜ä¸º: A -> S -> E
            
            prev_node = None
            if first_mem.prev_id:
                prev_node = await self.session.get(Memory, first_mem.prev_id)
                
            next_node = None
            if last_mem.next_id:
                next_node = await self.session.get(Memory, last_mem.next_id)
                
            if prev_node:
                prev_node.next_id = summary_mem.id
                summary_mem.prev_id = prev_node.id
                self.session.add(prev_node)
            
            # å¦‚æœ group åŒ…å«äº† next_node (é€»è¾‘ä¸Šä¸å¯èƒ½ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æŒ‰æ—¶é—´æ’åºçš„ä¸€ç»„)ï¼Œä½†ä¸ºäº†å®‰å…¨
            if next_node and next_node.id != summary_mem.id:
                next_node.prev_id = summary_mem.id
                summary_mem.next_id = next_node.id
                self.session.add(next_node)
                
            # å½’æ¡£åŸå§‹è®°å¿†
            archived_ids = []
            for m in group:
                m.type = "archived_event"
                self.session.add(m)
                archived_ids.append(m.id)
                
            # è®°å½•ç»´æŠ¤æ—¥å¿—
            record = MaintenanceRecord(
                consolidated=len(group),
                created_ids=json.dumps([summary_mem.id]),
                modified_data=json.dumps({"archived_ids": archived_ids})
            )
            self.session.add(record)
            
            # ç«‹å³æäº¤ï¼Œé‡Šæ”¾å†™é”ï¼Œé˜²æ­¢é•¿äº‹åŠ¡é˜»å¡
            await self.session.commit()
            print(f"[Reflection] Consolidated {len(group)} memories into ID {summary_mem.id}: {summary_text[:50]}...")
            
        print("[Reflection] Memory consolidation complete.")

    async def _generate_summary(self, llm: LLMService, memories: List[Memory], date_str: str) -> str:
        mem_text = "\n".join([f"- {m.realTime.split(' ')[1] if m.realTime else ''}: {m.content}" for m in memories])
        
        prompt = mdp.render("capabilities/reflection_summary", {
            "date_str": date_str,
            "mem_text": mem_text
        })
        
        try:
            res = await llm.chat([{"role": "user", "content": prompt}], temperature=0.3)
            return res["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"Summary generation failed: {e}")
            return None

    async def dream_and_associate(self, limit: int = 10) -> dict:
        """
        [æ¢¦å¢ƒæœºåˆ¶]
        æ‰«ææœ€è¿‘çš„æ— å…³è”è®°å¿†ï¼Œå°è¯•å‘ç°å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚
        å‡çº§ç‰ˆï¼šä½¿ç”¨å‘é‡æ£€ç´¢ + æ‰©æ•£æ¿€æ´» (Spreading Activation) å¯»æ‰¾æ½œåœ¨å…³è”
        """
        from services.memory_service import MemoryService
        
        print("[Reflection] Entering dream mode (scanning for associations)...", flush=True)
        
        # 1. è·å–æœ€è¿‘çš„ N æ¡è®°å¿† (Event ç±»å‹) ä½œä¸º"æ¢¦å¢ƒé”šç‚¹"
        statement = select(Memory).where(Memory.type == "event").order_by(desc(Memory.timestamp)).limit(limit)
        anchors = (await self.session.exec(statement)).all()
        
        if len(anchors) < 1:
            print("[Reflection] Not enough memories to associate.")
            return {"status": "skipped", "reason": "Not enough memories to associate"}

        config = await self._get_reflection_config()
        if not config["api_key"]:
            print("[Reflection] No API Key, skipping.")
            return {"status": "skipped", "reason": "No API Key configured"}

        llm = LLMService(
            api_key=config["api_key"],
            api_base=config["api_base"],
            model=config["model"]
        )

        # 2. é’ˆå¯¹æ¯ä¸ªé”šç‚¹ï¼Œä½¿ç”¨è®°å¿†æ£€ç´¢ç®—æ³•å¯»æ‰¾ç›¸å…³è®°å¿†
        # è¿™æ¯”ç®€å•çš„æ»‘åŠ¨çª—å£æ›´æ™ºèƒ½ï¼Œèƒ½å‘ç°è·¨åº¦å¾ˆå¤§çš„æ·±å±‚è”ç³»
        processed_pairs = set()
        new_relations_count = 0

        for target_memory in anchors:
            print(f"[Reflection] Dreaming about: {target_memory.content[:30]}...")
            
            # ä½¿ç”¨ MemoryService çš„é«˜çº§æ£€ç´¢ (Vector + Graph)
            # æ’é™¤æ‰è‡ªå·±ï¼Œä¸”ä¸é™åˆ¶æ—¶é—´èŒƒå›´ (exclude_after_time=None) ä»¥å…è®¸è¿æ¥è¿‡å»
            candidates = await MemoryService.get_relevant_memories(
                self.session, 
                target_memory.content, 
                limit=5
            )
            
            for candidate in candidates:
                if candidate.id == target_memory.id:
                    continue
                    
                # é¿å…é‡å¤å¤„ç†åŒä¸€å¯¹ (A, B) å’Œ (B, A)
                pair_key = tuple(sorted((target_memory.id, candidate.id)))
                if pair_key in processed_pairs:
                    continue
                processed_pairs.add(pair_key)

                # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²ç»å­˜åœ¨å…³è”
                existing = await self.session.exec(
                    select(MemoryRelation).where(
                        ((MemoryRelation.source_id == target_memory.id) & (MemoryRelation.target_id == candidate.id)) |
                        ((MemoryRelation.source_id == candidate.id) & (MemoryRelation.target_id == target_memory.id))
                    )
                )
                if existing.first():
                    continue # å·²å…³è”ï¼Œè·³è¿‡
            
                # 3. è°ƒç”¨ LLM åˆ¤æ–­å…³è”
                relation = await self._analyze_relation(llm, target_memory, candidate)
                
                if relation:
                    # 4. å†™å…¥æ•°æ®åº“
                    new_relation = MemoryRelation(
                        source_id=target_memory.id,
                        target_id=candidate.id,
                        relation_type=relation["type"],
                        strength=relation["strength"],
                        description=relation["description"]
                    )
                    self.session.add(new_relation)
                    await self.session.commit() # å‘ç°ä¸€ä¸ªå…³è”å°±æäº¤ä¸€ä¸ªï¼Œé¿å…é•¿äº‹åŠ¡
                    print(f"[Reflection] New association found: {relation['description']} (Strength: {relation['strength']})")
                    new_relations_count += 1
            
        print("[Reflection] Dream cycle complete.")
        return {"status": "success", "new_relations": new_relations_count, "anchors_processed": len(anchors)}

    async def scan_lonely_memories(self, limit: int = 5) -> dict:
        """
        [å­¤ç‹¬è®°å¿†æ‰«æå™¨]
        å¯»æ‰¾é‚£äº›æ²¡æœ‰å…³è” (MemoryRelation) çš„å­¤ç«‹è®°å¿†ï¼Œå¹¶å°è¯•å°†å®ƒä»¬ç»‡å…¥å…³ç³»ç½‘ã€‚
        """
        from services.memory_service import MemoryService
        
        print("[Reflection] Scanning for lonely memories...", flush=True)
        
        # 1. æŸ¥æ‰¾å­¤ç«‹è®°å¿† (æ²¡æœ‰ä½œä¸º source æˆ– target å‡ºç°åœ¨ Relation è¡¨ä¸­)
        # SQLModel ä¸æ”¯æŒç›´æ¥çš„ except/minusï¼Œæˆ‘ä»¬ç”¨ python é›†åˆå¤„ç† (å°æ•°æ®é‡å¯è¡Œ) æˆ–å­æŸ¥è¯¢
        # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œå…ˆæŸ¥å‡ºæ‰€æœ‰æœ‰å…³ç³»çš„ IDï¼Œå†æ’é™¤
        
        # è·å–æ‰€æœ‰æœ‰å…³ç³»çš„ ID
        rel_statement = select(MemoryRelation.source_id, MemoryRelation.target_id)
        relations = (await self.session.exec(rel_statement)).all()
        connected_ids = set()
        for src, tgt in relations:
            connected_ids.add(src)
            connected_ids.add(tgt)
            
        # æŸ¥æ‰¾ä¸åœ¨ connected_ids ä¸­çš„ Event è®°å¿†
        # ä¼˜å…ˆå¤„ç†æœ€è¿‘çš„å­¤ç‹¬è®°å¿†
        statement = select(Memory).where(Memory.type == "event").order_by(desc(Memory.timestamp))
        all_memories = (await self.session.exec(statement)).all()
        
        lonely_memories = [m for m in all_memories if m.id not in connected_ids][:limit]
        
        if not lonely_memories:
            print("[Reflection] No lonely memories found.")
            return {"status": "skipped", "reason": "No lonely memories found"}

        config = await self._get_reflection_config()
        if not config["api_key"]:
            print("[Reflection] No API Key, skipping.")
            return {"status": "skipped", "reason": "No API Key configured"}

        llm = LLMService(
            api_key=config["api_key"],
            api_base=config["api_base"],
            model=config["model"]
        )

        connections_found = 0

        # 2. ä¸ºæ¯ä¸ªå­¤ç‹¬è®°å¿†å¯»æ‰¾å½’å®¿
        for lonely_mem in lonely_memories:
            print(f"[Reflection] Trying to connect lonely memory: {lonely_mem.content[:30]}...")
            
            # ä½¿ç”¨å‘é‡æ£€ç´¢å¯»æ‰¾ç›¸ä¼¼è®°å¿†
            candidates = await MemoryService.get_relevant_memories(
                self.session, 
                lonely_mem.content, 
                limit=5
            )
            
            for candidate in candidates:
                if candidate.id == lonely_mem.id:
                    continue
                
                # åŒé‡æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…³è” (è™½ç„¶å‰é¢è¿‡æ»¤äº†ï¼Œä½†ä¸ºäº†å¹¶å‘å®‰å…¨)
                existing = await self.session.exec(
                    select(MemoryRelation).where(
                        ((MemoryRelation.source_id == lonely_mem.id) & (MemoryRelation.target_id == candidate.id)) |
                        ((MemoryRelation.source_id == candidate.id) & (MemoryRelation.target_id == lonely_mem.id))
                    )
                )
                if existing.first():
                    continue

                # åˆ†æå…³è”
                relation = await self._analyze_relation(llm, lonely_mem, candidate)
                
                if relation:
                    new_relation = MemoryRelation(
                        source_id=lonely_mem.id,
                        target_id=candidate.id,
                        relation_type=relation["type"],
                        strength=relation["strength"],
                        description=relation["description"]
                    )
                    self.session.add(new_relation)
                    await self.session.commit()
                    print(f"[Reflection] Connected lonely memory! {relation['description']}")
                    connections_found += 1
                    # æ‰¾åˆ°ä¸€ä¸ªå…³è”å°±è·³å‡ºå½“å‰å€™é€‰å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå­¤ç‹¬è®°å¿† (é¿å…è¿‡åº¦è¿æ¥)
                    # æˆ–è€…ä¹Ÿå¯ä»¥ç»§ç»­æ‰¾ï¼Œçœ‹ç­–ç•¥ã€‚è¿™é‡Œé€‰æ‹©ç»§ç»­æ‰¾ï¼Œç»‡ç½‘è¶Šå¯†è¶Šå¥½ã€‚
        
        await self.session.commit()
        print(f"[Reflection] Lonely memory scan complete. Processed {len(lonely_memories)} items.")
        return {"status": "success", "processed_count": len(lonely_memories), "connections_found": connections_found}

    async def _analyze_relation(self, llm: LLMService, m1: Memory, m2: Memory) -> Optional[dict]:
        """è®© LLM åˆ†æä¸¤æ¡è®°å¿†çš„å…³ç³»"""
        prompt = mdp.render("services/memory/reflection/relation", {
            "m1_time": m1.realTime,
            "m1_content": m1.content,
            "m1_tags": m1.tags,
            "m2_time": m2.realTime,
            "m2_content": m2.content,
            "m2_tags": m2.tags
        })
        
        try:
            # [Fix] GLM-4 ç­‰éƒ¨åˆ†æ¨¡å‹ä¸æ”¯æŒ response_format="json_object"ï¼Œç§»é™¤è¯¥å‚æ•°ä»¥å…¼å®¹
            # æ˜¾å¼è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 300s (5åˆ†é’Ÿ)ï¼Œé˜²æ­¢åæ€åˆ†æè¶…æ—¶
            response = await llm.chat([{"role": "user", "content": prompt}], temperature=0.1, timeout=300.0)
            content = response["choices"][0]["message"]["content"]
            
            # Parse JSON (Simple)
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            if data.get("has_relation"):
                return data
            return None
        except Exception as e:
            print(f"[Reflection] Error analyzing relation: {e}")
            return None
