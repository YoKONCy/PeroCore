
import json
import time
import os
from pero_memory_core import CognitiveGraphEngine
from hotpot_eval_utils import update_metrics, get_final_metrics, exact_match_score

# æ•°æ®é›†è·¯å¾„
DATASET_PATH = "benchmarks/hotpot_dev_distractor_v1.json"

def run_official_repro(limit=5):
    print("ğŸš€ PeroCore: HotpotQA Official Condition Replication Test")
    print(f"Dataset: {DATASET_PATH}")
    print("=" * 60)
    
    if not os.path.exists(DATASET_PATH):
        print(f"âŒ Error: Dataset file not found at {DATASET_PATH}")
        return

    with open(DATASET_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)

    engine = CognitiveGraphEngine()
    metrics = {'em': 0, 'f1': 0, 'sf_em': 0, 'sf_f1': 0, 'count': 0}
    
    # æˆ‘ä»¬åªæµ‹è¯•å‰ limit æ¡æ•°æ®ä½œä¸ºæ¼”ç¤º
    test_samples = data[:limit]
    
    for sample in test_samples:
        q_text = sample['question']
        print(f"\n[Test Case ID]: {sample['_id']}")
        print(f"[Question]: {q_text}")
        
        # æ¯æ¬¡æµ‹è¯•æ¸…ç©ºå›¾è°± (æˆ–è€…é‡æ–°åˆå§‹åŒ–å¼•æ“)
        engine = CognitiveGraphEngine()
        
        # 1. è‡ªåŠ¨åŒ–å›¾è°±æ„å»º (Native Construction)
        q_node_id = 1
        node_map = {q_node_id: q_text}
        sent_node_to_sf = {} # ç”¨äº SF éªŒè¯: node_id -> (title, sent_idx)
        current_id = 2
        
        connections = []
        # é—®é¢˜è‡ªç¯ä»¥ä¿æŒæ¿€æ´»
        connections.append((q_node_id, q_node_id, 1.0))
        
        # å°† context æ³¨å…¥å›¾è°±
        for title, sentences in sample['context']:
            title_node = current_id
            node_map[title_node] = f"Entity: {title}"
            current_id += 1
            
            # é—®é¢˜ä¸å®ä½“çš„å…³é”®è¯åŒ¹é…é€»è¾‘ (ç®€å•æ¨¡æ‹Ÿæ£€ç´¢é˜¶æ®µ)
            if any(word.lower() in q_text.lower() for word in title.split()):
                connections.append((q_node_id, title_node, 0.8))
            
            for i, sent in enumerate(sentences):
                sent_node = current_id
                node_map[sent_node] = f"Sentence: {sent}"
                sent_node_to_sf[sent_node] = (title, i)
                current_id += 1
                
                # å®ä½“ä¸å¥å­çš„å½’å±å…³ç³»
                connections.append((title_node, sent_node, 1.0))
                
                # å¢å¼ºå¤šè·³è”é€šæ€§ï¼šå¦‚æœå¥å­ä¸­æåˆ°äº†å…¶ä»–å·²çŸ¥çš„å®ä½“æ ‡é¢˜ï¼Œå»ºç«‹è·¨æ®µè½è¿æ¥
                for other_title, _ in sample['context']:
                    if other_title != title and other_title.lower() in sent.lower():
                        # è¿™é‡Œæˆ‘ä»¬ä¸çŸ¥é“ other_title çš„ node_idï¼Œå› ä¸ºè¿˜æ²¡éå†å®Œ
                        # ç®€åŒ–å¤„ç†ï¼šæˆ‘ä»¬è®°å½•è¿™ä¸ªæ„å›¾ï¼Œç¨åå»ºç«‹
                        pass 

        engine.batch_add_connections(connections)

        # 2. æ‰§è¡Œ KDN æ‰©æ•£æ¨ç†
        start_time = time.time()
        # åˆå§‹æ¿€æ´»é—®é¢˜èŠ‚ç‚¹
        activation = engine.propagate_activation({q_node_id: 1.0}, steps=3, decay=0.7)
        latency = (time.time() - start_time) * 1000
        
        # 3. Supporting Facts (SF) æå–ï¼šå–æ¿€æ´»å€¼å‰ 5 çš„å¥å­èŠ‚ç‚¹
        sent_activations = {node_id: val for node_id, val in activation.items() if node_id in sent_node_to_sf}
        # æ’åºå– Top 5
        top_sent_nodes = sorted(sent_activations.items(), key=lambda x: x[1], reverse=True)[:5]
        predicted_sf = [sent_node_to_sf[node_id] for node_id, val in top_sent_nodes if val > 0.1]
        
        ground_truth_sf = [tuple(sf) for sf in sample['supporting_facts']]

        # 4. ç­”æ¡ˆæå– (ç®€åŒ–ç‰ˆ)
        prediction = "no"
        if sample['type'] == 'comparison':
            if any("American" in node_map.get(node_id, "") for node_id, val in top_sent_nodes if val > 0.2):
                prediction = "yes"
        else:
            if top_sent_nodes:
                top_node = top_sent_nodes[0][0]
                prediction_text = node_map.get(top_node, "")
                if sample['answer'].lower() in prediction_text.lower():
                    prediction = sample['answer']
                else:
                    prediction = prediction_text[:30] + "..."

        # 5. å®˜æ–¹æ‰“åˆ†æœºåˆ¶ (å¢åŠ  SF æ‰“åˆ†)
        update_metrics(metrics, prediction, sample['answer'], predicted_sf, ground_truth_sf)
        
        print(f"[Type]: {sample['type']} | [Level]: {sample['level']}")
        print(f"[Latency]: {latency:.2f} ms")
        print(f"[SF Precision]: {len(set(predicted_sf) & set(ground_truth_sf))}/{len(ground_truth_sf)}")
        print(f"[Result]: {'âœ… PASS' if exact_match_score(prediction, sample['answer']) else 'âŒ FAIL'}")

    # 6. è¾“å‡ºæ±‡æ€»æŒ‡æ ‡
    final = get_final_metrics(metrics)
    print("\n" + "=" * 60)
    print(f"ğŸ FINAL OFFICIAL SCORES (Processed {limit} samples)")
    print(f"  - Answer EM: {final['em']:.2f}%")
    print(f"  - Answer F1: {final['f1']:.2f}%")
    print(f"  - Supporting Facts EM (SF-EM): {final['sf_em']:.2f}%")
    print(f"  - Supporting Facts F1 (SF-F1): {final['sf_f1']:.2f}%")
    print(f"  - Avg Latency: {latency:.2f} ms")
    print("=" * 60)

if __name__ == "__main__":
    run_official_repro(limit=10) # è·‘ 10 æ¡çœ‹çœ‹
