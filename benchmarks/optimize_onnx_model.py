"""
ONNX æ¨¡å‹ä¼˜åŒ–è„šæœ¬

é’ˆå¯¹ AuraVision æ¨¡å‹è¿›è¡Œä»¥ä¸‹ä¼˜åŒ–:
1. å›¾ç®€åŒ– (Graph Simplification)
2. ç®—å­èåˆ (Operator Fusion)
3. å¸¸é‡æŠ˜å  (Constant Folding)
4. å¯é€‰: FP16 åŠç²¾åº¦è½¬æ¢

ä½¿ç”¨æ–¹æ³•:
    cd PeroCore/backend
    pip install onnx onnxsim onnxoptimizer
    python ../benchmarks/optimize_onnx_model.py

ä¾èµ–:
    - onnx
    - onnxsim (onnx-simplifier)
    - onnxoptimizer
"""

import os
import sys
from pathlib import Path

# æ¨¡å‹è·¯å¾„
BACKEND_DIR = Path(__file__).parent.parent / "backend"
MODEL_DIR = BACKEND_DIR / "models" / "AuraVision" / "weights"
INPUT_MODEL = MODEL_DIR / "auravision_v1.onnx"
OUTPUT_MODEL = MODEL_DIR / "auravision_v1_optimized.onnx"
OUTPUT_MODEL_FP16 = MODEL_DIR / "auravision_v1_fp16.onnx"


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing = []
    
    try:
        import onnx
    except ImportError:
        missing.append("onnx")
    
    try:
        import onnxsim
    except ImportError:
        missing.append("onnxsim")
    
    try:
        import onnxoptimizer
    except ImportError:
        missing.append("onnxoptimizer")
    
    if missing:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print(f"   è¯·è¿è¡Œ: pip install {' '.join(missing)}")
        return False
    
    return True


def get_model_info(model_path: Path) -> dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    import onnx
    
    if not model_path.exists():
        return None
    
    model = onnx.load(str(model_path))
    graph = model.graph
    
    # ç»Ÿè®¡ç®—å­æ•°é‡
    op_counts = {}
    for node in graph.node:
        op_counts[node.op_type] = op_counts.get(node.op_type, 0) + 1
    
    # è®¡ç®—æ¨¡å‹å¤§å°
    file_size_mb = model_path.stat().st_size / (1024 * 1024)
    
    return {
        "file_size_mb": file_size_mb,
        "num_nodes": len(graph.node),
        "num_inputs": len(graph.input),
        "num_outputs": len(graph.output),
        "op_counts": op_counts
    }


def print_model_info(info: dict, label: str):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    print(f"\nğŸ“Š {label}")
    print(f"   æ–‡ä»¶å¤§å°: {info['file_size_mb']:.2f} MB")
    print(f"   èŠ‚ç‚¹æ•°é‡: {info['num_nodes']}")
    print(f"   è¾“å…¥æ•°é‡: {info['num_inputs']}")
    print(f"   è¾“å‡ºæ•°é‡: {info['num_outputs']}")
    print(f"   ç®—å­åˆ†å¸ƒ: {dict(sorted(info['op_counts'].items(), key=lambda x: -x[1])[:5])}")


def simplify_model(input_path: Path, output_path: Path) -> bool:
    """ä½¿ç”¨ onnx-simplifier ç®€åŒ–æ¨¡å‹"""
    import onnx
    import onnxsim
    
    print(f"\nâ³ æ­£åœ¨ç®€åŒ–æ¨¡å‹...")
    
    try:
        model = onnx.load(str(input_path))
        
        # ç®€åŒ–
        model_simp, check = onnxsim.simplify(
            model,
            skip_fuse_bn=False,  # èåˆ BatchNorm
            skip_constant_folding=False,  # å¸¸é‡æŠ˜å 
            skip_shape_inference=False,  # å½¢çŠ¶æ¨æ–­
        )
        
        if check:
            onnx.save(model_simp, str(output_path))
            print(f"âœ… æ¨¡å‹ç®€åŒ–æˆåŠŸ: {output_path}")
            return True
        else:
            print("âŒ ç®€åŒ–åçš„æ¨¡å‹éªŒè¯å¤±è´¥")
            return False
    
    except Exception as e:
        print(f"âŒ ç®€åŒ–å¤±è´¥: {e}")
        return False


def optimize_model(input_path: Path, output_path: Path) -> bool:
    """ä½¿ç”¨ onnxoptimizer è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–"""
    import onnx
    import onnxoptimizer
    
    print(f"\nâ³ æ­£åœ¨è¿›è¡Œå›¾ä¼˜åŒ–...")
    
    try:
        model = onnx.load(str(input_path))
        
        # å¯ç”¨çš„ä¼˜åŒ– pass
        passes = [
            'eliminate_deadend',
            'eliminate_identity',
            'eliminate_nop_dropout',
            'eliminate_nop_pad',
            'eliminate_nop_transpose',
            'eliminate_unused_initializer',
            'fuse_add_bias_into_conv',
            'fuse_bn_into_conv',
            'fuse_consecutive_concats',
            'fuse_consecutive_squeezes',
            'fuse_consecutive_transposes',
            'fuse_matmul_add_bias_into_gemm',
            'fuse_pad_into_conv',
            'fuse_transpose_into_gemm',
        ]
        
        optimized_model = onnxoptimizer.optimize(model, passes)
        onnx.save(optimized_model, str(output_path))
        
        print(f"âœ… å›¾ä¼˜åŒ–å®Œæˆ: {output_path}")
        return True
    
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å¤±è´¥: {e}")
        return False


def convert_to_fp16(input_path: Path, output_path: Path) -> bool:
    """è½¬æ¢ä¸º FP16 åŠç²¾åº¦"""
    import onnx
    from onnx import numpy_helper, TensorProto
    
    print(f"\nâ³ æ­£åœ¨è½¬æ¢ä¸º FP16...")
    
    try:
        model = onnx.load(str(input_path))
        
        # éå†æ‰€æœ‰åˆå§‹åŒ–å™¨ (æƒé‡)
        for initializer in model.graph.initializer:
            if initializer.data_type == TensorProto.FLOAT:
                # è½¬æ¢ä¸º FP16
                np_array = numpy_helper.to_array(initializer)
                np_array_fp16 = np_array.astype('float16')
                new_initializer = numpy_helper.from_array(np_array_fp16, initializer.name)
                initializer.CopyFrom(new_initializer)
        
        onnx.save(model, str(output_path))
        print(f"âœ… FP16 è½¬æ¢å®Œæˆ: {output_path}")
        return True
    
    except Exception as e:
        print(f"âŒ FP16 è½¬æ¢å¤±è´¥: {e}")
        return False


def main():
    print("=" * 60)
    print("ğŸ”§ ONNX æ¨¡å‹ä¼˜åŒ–å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ£€æŸ¥è¾“å…¥æ¨¡å‹
    if not INPUT_MODEL.exists():
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_MODEL}")
        print("   è¯·å…ˆå¯¼å‡º AuraVision æ¨¡å‹ä¸º ONNX æ ¼å¼")
        return
    
    # åŸå§‹æ¨¡å‹ä¿¡æ¯
    original_info = get_model_info(INPUT_MODEL)
    print_model_info(original_info, "åŸå§‹æ¨¡å‹")
    
    # æ­¥éª¤ 1: ç®€åŒ–æ¨¡å‹
    temp_path = MODEL_DIR / "temp_simplified.onnx"
    if not simplify_model(INPUT_MODEL, temp_path):
        return
    
    # æ­¥éª¤ 2: è¿›ä¸€æ­¥ä¼˜åŒ–
    if not optimize_model(temp_path, OUTPUT_MODEL):
        # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åçš„æ¨¡å‹
        import shutil
        shutil.copy(temp_path, OUTPUT_MODEL)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if temp_path.exists():
        temp_path.unlink()
    
    # ä¼˜åŒ–åæ¨¡å‹ä¿¡æ¯
    optimized_info = get_model_info(OUTPUT_MODEL)
    print_model_info(optimized_info, "ä¼˜åŒ–åæ¨¡å‹")
    
    # è®¡ç®—èŠ‚çœ
    size_reduction = (1 - optimized_info['file_size_mb'] / original_info['file_size_mb']) * 100
    node_reduction = (1 - optimized_info['num_nodes'] / original_info['num_nodes']) * 100
    
    print(f"\nğŸ“ˆ ä¼˜åŒ–æ•ˆæœ:")
    print(f"   æ–‡ä»¶å¤§å°å‡å°‘: {size_reduction:.1f}%")
    print(f"   èŠ‚ç‚¹æ•°é‡å‡å°‘: {node_reduction:.1f}%")
    
    # å¯é€‰: FP16 è½¬æ¢ (è‡ªåŠ¨æ‰§è¡Œ)
    print("\n" + "-" * 60)
    print("ç”Ÿæˆ FP16 åŠç²¾åº¦ç‰ˆæœ¬...")
    convert_to_fp16(OUTPUT_MODEL, OUTPUT_MODEL_FP16)
    fp16_info = get_model_info(OUTPUT_MODEL_FP16)
    if fp16_info:
        print_model_info(fp16_info, "FP16 æ¨¡å‹")
    
    print("\n" + "=" * 60)
    print("âœ… ä¼˜åŒ–å®Œæˆ!")
    print("=" * 60)
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"   1. æ›´æ–° aura_vision.rs ä¸­çš„æ¨¡å‹è·¯å¾„")
    print(f"   2. é‡æ–°ç¼–è¯‘ Rust æ¨¡å—: maturin build --release")
    print(f"   3. é‡æ–°è¿è¡Œ benchmark éªŒè¯æ€§èƒ½æå‡")


if __name__ == "__main__":
    main()
