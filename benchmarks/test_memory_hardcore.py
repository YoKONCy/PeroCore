import sys
import os
import json
import random
import numpy as np
import asyncio
from datetime import datetime, timedelta
from sqlmodel import SQLModel, Session, create_engine, select, Field
from typing import List, Dict, Optional

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# --- ç®€åŒ–çš„æ¨¡å‹å®šä¹‰ (ä¸ºäº†ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ– backend å¤æ‚çš„ import) ---
# æˆ‘ä»¬é‡æ–°å®šä¹‰æœ€å°åŒ–çš„æ¨¡å‹ï¼Œä»¥å…å¼•å…¥ä¸å¿…è¦çš„ä¾èµ–æŠ¥é”™

def get_local_now():
    return datetime.now()

def get_local_timestamp():
    return datetime.now().timestamp() * 1000

class Memory(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    content: str
    tags: str = ""
    importance: int = 1
    timestamp: float = Field(default_factory=get_local_timestamp)
    embedding_json: str = "[]" 

class MemoryRelation(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    source_id: int = Field(index=True)
    target_id: int = Field(index=True)
    relation_type: str = "associative"
    strength: float = 0.5
    created_at: datetime = Field(default_factory=get_local_now)

# --- æ¨¡æ‹ŸæœåŠ¡ ---

class MockEmbeddingService:
    def __init__(self):
        self.dim = 384
        # é¢„å®šä¹‰ä¸€äº›ç°‡ä¸­å¿ƒ (éšæœºå‘é‡)
        np.random.seed(42)
        self.clusters = {
            "preparation": np.random.rand(384) - 0.5,
            "beach": np.random.rand(384) - 0.5,
            "food": np.random.rand(384) - 0.5,
            "accident": np.random.rand(384) - 0.5,
        }
    
    def encode_one(self, text: str) -> List[float]:
        # ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥å†³å®šå‘é‡ä½ç½®
        base = (np.random.rand(384) - 0.5) * 0.1 # å™ªå£°
        text_lower = text.lower()
        
        if any(w in text_lower for w in ["pack", "ticket", "ready", "flight", "airport"]):
            base += self.clusters["preparation"]
        if any(w in text_lower for w in ["swim", "sand", "sun", "sea", "beach", "ocean"]):
            base += self.clusters["beach"]
        if any(w in text_lower for w in ["eat", "food", "delicious", "seafood", "fish", "shrimp"]):
            base += self.clusters["food"]
        if any(w in text_lower for w in ["lost", "rain", "hurt", "delay", "broke"]):
            base += self.clusters["accident"]
            
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(base)
        if norm > 0:
            base = base / norm
        return base.tolist()

class MockReranker:
    def compute_score(self, query: str, doc: str) -> float:
        # ç®€å•çš„å…³é”®è¯é‡å æ‰“åˆ†
        q_words = set(query.lower().split())
        d_words = set(doc.lower().replace("(", "").replace(")", "").split())
        if not q_words: return 0.0
        overlap = len(q_words.intersection(d_words))
        return overlap / len(q_words)

# --- ä¸»æµ‹è¯•é€»è¾‘ ---

async def run_hardcore_test():
    print("ğŸ”¥ PeroCore è®°å¿†ç³»ç»Ÿç¡¬æ ¸å‹åŠ›æµ‹è¯•")
    print("=" * 60)

    # 1. åˆå§‹åŒ–æ•°æ®åº“ (In-Memory SQLite)
    engine = create_engine("sqlite:///:memory:")
    SQLModel.metadata.create_all(engine)
    
    embedding_service = MockEmbeddingService()
    reranker = MockReranker()
    
    # 2. ç”Ÿæˆ 100 æ¡è®°å¿† (The Story of a Beach Trip)
    print("[Phase 1] ç”Ÿæˆ 100 æ¡è®°å¿†ç½‘ç»œ...")
    
    base_time = datetime.now() - timedelta(days=10)
    
    # å®šä¹‰æ•…äº‹çº¿ç‰‡æ®µ
    story_clusters = [
        ("preparation", [
            "Buying plane tickets to Hawaii", "Packing swimsuits and sunscreen", 
            "Checking passport validity", "Booking the hotel with sea view",
            "Asking neighbor to water plants", "Driving to the airport",
            "Checking in luggage", "Waiting at the gate", "Boarding the plane",
            "Watching movies on flight"
        ]),
        ("beach", [
            "Arrived at the sunny beach", "The ocean is so blue", 
            "Building a huge sandcastle", "Swimming in the cool water",
            "Sunbathing on the deck chair", "Playing beach volleyball",
            "Collecting seashells", "Watching the sunset", "Taking photos by the sea",
            "Surfing for the first time"
        ]),
        ("food", [
            "Eating fresh lobster at a local restaurant", "Drinking coconut water",
            "Trying the famous shrimp taco", "Having ice cream for dessert",
            "Breakfast buffet at the hotel", "Spicy seafood soup",
            "Grilled fish on the beach", "Tropical fruit platter",
            "Drinking cocktails at the bar", "Late night snacks"
        ]),
        ("accident", [
            "Suddenly started raining heavily", "Forgot the umbrella",
            "Lost the room key card", "Got a minor sunburn",
            "Mosquito bites are itchy", "Flight delayed on return",
            "Traffic jam to the airport", "Luggage handle broke",
            "Phone battery died", "Forgot to buy souvenirs"
        ])
    ]
    
    generated_ids = []
    
    with Session(engine) as session:
        count = 0
        for cluster_name, texts in story_clusters:
            for text in texts:
                # æ¯ä¸ªæ ¸å¿ƒæ–‡æœ¬ç”Ÿæˆ 2-3 ä¸ªå˜ä½“ä»¥å¡«å……æ•°é‡
                for i in range(3): 
                    content = f"{text} (Detail {i})"
                    vec = embedding_service.encode_one(content)
                    mem = Memory(
                        content=content,
                        tags=cluster_name,
                        importance=random.randint(1, 10),
                        timestamp=base_time.timestamp() + count * 3600, # æ¯å°æ—¶ä¸€æ¡
                        embedding_json=json.dumps(vec)
                    )
                    session.add(mem)
                    session.commit() # Commit to get ID
                    session.refresh(mem)
                    generated_ids.append((mem.id, cluster_name))
                    count += 1
                    if count >= 100: break
            if count >= 100: break
            
        print(f"âœ… å·²å­˜å…¥ {count} æ¡è®°å¿†")
        
        # 3. æ„å»ºå…³ç³»ç½‘ç»œ (æ¨¡æ‹Ÿ LLM æ€»ç»“)
        print("[Phase 2] æ„å»ºè®°å¿†å…³è”å›¾è°±...")
        
        # æŒ‰ Cluster åˆ†ç»„
        clusters = {}
        for mid, c_name in generated_ids:
            if c_name not in clusters: clusters[c_name] = []
            clusters[c_name].append(mid)
            
        # Cluster å†…éƒ¨è¿æ¥ (éšæœºé€‰å–ä¸€äº›è¾¹ï¼Œé¿å…å®Œå…¨å›¾å¤ªå¯†é›†)
        rel_count = 0
        for c_name, ids in clusters.items():
            for i in range(len(ids)):
                # è¿æ¥åˆ°åŒç°‡çš„å…¶ä»– 3 ä¸ªèŠ‚ç‚¹
                targets = random.sample(ids, min(len(ids), 4))
                for tid in targets:
                    if tid != ids[i]:
                        session.add(MemoryRelation(
                            source_id=ids[i], target_id=tid, 
                            strength=random.uniform(0.7, 0.9), relation_type="associative"
                        ))
                        rel_count += 1
        
        # Cluster ä¹‹é—´è¿æ¥ (æ¨¡æ‹Ÿæ•…äº‹æ¨è¿›)
        # Prep -> Beach -> Food -> Accident
        flow = ["preparation", "beach", "food", "accident"]
        for i in range(len(flow)-1):
            src_c = flow[i]
            dst_c = flow[i+1]
            # éšæœºè¿æ¥ 10 æ¡è¾¹
            src_ids = clusters[src_c]
            dst_ids = clusters[dst_c]
            for _ in range(10):
                s = random.choice(src_ids)
                t = random.choice(dst_ids)
                session.add(MemoryRelation(
                    source_id=s, target_id=t,
                    strength=0.5, relation_type="sequential"
                ))
                rel_count += 1
                
        session.commit()
        print(f"âœ… å·²å»ºç«‹ {rel_count} æ¡å…³è”")

        # 4. æ¨¡æ‹Ÿæ£€ç´¢å…¨æµç¨‹
        print("\n[Phase 3] æ¨¡æ‹Ÿå¯¹è¯æ£€ç´¢...")
        query = "Do you remember the delicious seafood we had?"
        print(f"ğŸ—£ï¸ ç”¨æˆ·æé—®: \"{query}\"")
        
        # Step A: å‘é‡æ£€ç´¢ (Vector Search)
        print("   -> 1. æ‰§è¡Œå‘é‡æ£€ç´¢ (Top-20)...")
        query_vec = embedding_service.encode_one(query)
        
        # ç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
        candidates = []
        all_mems = session.exec(select(Memory)).all()
        for m in all_mems:
            m_vec = json.loads(m.embedding_json)
            sim = np.dot(query_vec, m_vec) # å‡è®¾å·²å½’ä¸€åŒ–
            candidates.append((m.id, sim))
            
        candidates.sort(key=lambda x: x[1], reverse=True)
        top_20 = candidates[:20]
        
        # Step B: æ‰©æ•£æ¿€æ´» (Spreading Activation)
        print("   -> 2. æ‰§è¡Œæ‰©æ•£æ¿€æ´» (Rust Engine)...")
        try:
            from pero_memory_core import CognitiveGraphEngine
            engine = CognitiveGraphEngine()
            
            # åŠ è½½å›¾è°±
            all_rels = session.exec(select(MemoryRelation)).all()
            rust_rels = [(r.source_id, r.target_id, r.strength) for r in all_rels]
            engine.batch_add_connections(rust_rels)
            
            # åˆå§‹æ¿€æ´»
            initial_scores = {mid: float(score) for mid, score in top_20}
            
            # æ‰©æ•£
            activated_scores = engine.propagate_activation(
                initial_scores, steps=2, decay=0.5, min_threshold=0.01
            )
            print(f"      æ‰©æ•£å‰èŠ‚ç‚¹æ•°: {len(initial_scores)}")
            print(f"      æ‰©æ•£åèŠ‚ç‚¹æ•°: {len(activated_scores)}")
            
        except ImportError:
            print("âŒ Rust å¼•æ“æœªæ‰¾åˆ°ï¼Œæ— æ³•è¿›è¡Œæ‰©æ•£æµ‹è¯•ï¼")
            return False
            
        # Step C: æ··åˆæ’åºä¸ Rerank
        print("   -> 3. æ‰§è¡Œæ··åˆæ’åºä¸ Rerank...")
        final_candidates = []
        for mid, score in activated_scores.items():
            mem = session.get(Memory, mid)
            if not mem: continue
            
            # æ··åˆåˆ†æ•° = æ¿€æ´»åˆ†æ•° * 0.7 + Rerankåˆ†æ•° * 0.3
            rerank_score = reranker.compute_score(query, mem.content)
            final_score = score * 0.7 + rerank_score * 0.3
            final_candidates.append((mem, final_score, score, rerank_score))
            
        final_candidates.sort(key=lambda x: x[1], reverse=True)
        top_10 = final_candidates[:10]
        
        # 5. ç»“æœå±•ç¤ºä¸è‡ªæˆ‘è¯„ä¼°
        report_lines = []
        report_lines.append("\n[Phase 4] æ£€ç´¢ç»“æœè¯„ä¼°")
        report_lines.append("-" * 60)
        for i, (mem, f_score, a_score, r_score) in enumerate(top_10):
            report_lines.append(f"{i+1}. [Score: {f_score:.4f}] {mem.content}")
            report_lines.append(f"   (Activation: {a_score:.4f}, Rerank: {r_score:.4f}, Tags: {mem.tags})")
            
        # éªŒè¯é€»è¾‘
        food_count = sum(1 for m, _, _, _ in top_10 if "food" in m.tags)
        report_lines.append("-" * 60)
        report_lines.append(f"ğŸ“Š ç»Ÿè®¡: Top-10 ä¸­æœ‰ {food_count} æ¡å…³äº 'food' çš„è®°å¿†")
        
        if food_count >= 3:
            report_lines.append("âœ… æˆåŠŸå¬å›å¤§é‡ç›¸å…³è®°å¿†")
        else:
            report_lines.append("âš ï¸ å¬å›ç›¸å…³æ€§ä¸è¶³")
            
        diffusion_wins = []
        for m, f, a, r in top_10:
            if r < 0.1 and a > 0.2: 
                diffusion_wins.append(m.content)
                
        if diffusion_wins:
            report_lines.append(f"âœ¨ å‘ç°æ‰©æ•£æƒŠå–œ (éå…³é”®è¯åŒ¹é…):")
            for c in diffusion_wins:
                report_lines.append(f"   - {c}")
        else:
            report_lines.append("   (æœ¬æ¬¡æœªå‘ç°æ˜æ˜¾çš„éå…³é”®è¯æ‰©æ•£æƒŠå–œ)")

        # å†™å…¥æ–‡ä»¶
        with open("hardcore_report.md", "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        
        print("âœ… æµ‹è¯•å®Œæˆï¼Œç»“æœå·²å†™å…¥ hardcore_report.md")
        return True

if __name__ == "__main__":
    asyncio.run(run_hardcore_test())
